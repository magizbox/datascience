{
    "docs": [
        {
            "location": "/", 
            "text": "Machine Learning\n\n\n\n\nMachine learning is a branch of science that deals with programming the systems in such a way that they automatically learn and improve with experience. Here, learning means recognizing and understanding the input data and making wise decisions based on the supplied data.\n\n\n\n\nBooks\n\n\n\n\n\n\n\n\nCourses\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\n\n\nConferences\n\n\nICML - Internal Conference on Machine Learning\n\n\nThe International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning, attracting annually about 500 participants from all over the world. It is supported by the International Machine Learning Society (IMLS). The conference attracts leading innovations in the field of machine learning.\n\n\nNIPS - Annual Conference on Neural Information Processing \n\n\nThe Conference and Workshop on Neural Information Processing Systems (NIPS) is a machine learning and computational neuroscience conference held every December. The conference is a single track meeting that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts. According to Microsoft Academic Search, NIPS is the top conference on machine learning.\n\n\nNIPS Deep Learning Workshop \n\n\n2015, \n2014\n, \n2013\n, \n2012\n, \n2011\n, \n2010\n\n\nThe Deep Learning and Representation Learning Workshop will be held in conjunction with Neural Information Processing Systems (NIPS)\n\n\nDeep Learning algorithms attempt to discover good representations, at multiple levels of abstraction. There has been rapid progress in this area in recent years, both in terms of algorithms and in terms of applications, but many challenges remain. The workshop aims at bringing together researchers in that field and discussing these challenges, brainstorming about new solutions.\n\n\nACM SIGKDD\n\n\n2015\n, \n2014\n, \n2013\n\n\nKDD provides the premier forum for advancement and adoption of the \"science\" of knowledge discovery and data mining. KDD encourages:\n\n\n\n    \nResearch in KDD (through annual research conferences, newsletter and other related activities)\nAdoption of \"standards\" in the market in terms of terminology, evaluation, methodology\nInterdisciplinary education among KDD researchers, practitioners, and users\nKDD activities include the annual Conference on Knowledge Discovery and Data Mining and the SIGKDD Explorations Newsletter.\n\n\n\n\n\nJournals\n\n\nJMLR - Journal of Machine Learning Research\n\n\nThe Journal of Machine Learning Research (JMLR) provides an international forum for the electronic and paper publication of high-quality scholarly articles in all areas of machine learning. All published papers are freely available online.\n\n\nJAIR - Journal of Artificial Intelligence Research\n\n\nJAIR(ISSN 1076 - 9757) covers all areas of artificial intelligence (AI), publishing refereed research articles, survey articles, and technical notes. Established in 1993 as one of the first electronic scientific journals, JAIR is indexed by INSPEC, Science Citation Index, and MathSciNet. JAIR reviews papers within approximately three months of submission and publishes accepted articles on the internet immediately upon receiving the final versions. JAIR articles are published for free distribution on the internet by the AI Access Foundation, and for purchase in bound volumes by AAAI Press.\n\n\nGroups and Social Networks\n\n\nYou can join many peer groups - see \nTop 30 LinkedIn Groups for Analytics, Big Data, Data Mining, and Data Science\n.\n\n\nAnalyticBridge \nis an active community for Analytics and Data Science.\n\n\nYou can attend some of the many \nMeetings and Conferences on Analytics, Big Data, Data Mining, Data Science, \n Knowledge Discovery\n.\n\n\nPeople\n\n\nJure Leskovec\n: \n#SocialNetworkAnalysis\n\n\nAndrew Ng\n: \n#DeepLearning\n\n\nDavid Blei\n: \n#TopicModel\n\n\nMatei Zaharia\n: \n#apachespark , #2013, \ncreator of apache spark, (interview \n#1\n)", 
            "title": "Home"
        }, 
        {
            "location": "/#machine-learning", 
            "text": "Machine learning is a branch of science that deals with programming the systems in such a way that they automatically learn and improve with experience. Here, learning means recognizing and understanding the input data and making wise decisions based on the supplied data.", 
            "title": "Machine Learning"
        }, 
        {
            "location": "/#books", 
            "text": "", 
            "title": "Books"
        }, 
        {
            "location": "/#courses", 
            "text": "", 
            "title": "Courses"
        }, 
        {
            "location": "/#videos", 
            "text": "", 
            "title": "Videos"
        }, 
        {
            "location": "/#conferences", 
            "text": "ICML - Internal Conference on Machine Learning  The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning, attracting annually about 500 participants from all over the world. It is supported by the International Machine Learning Society (IMLS). The conference attracts leading innovations in the field of machine learning.  NIPS - Annual Conference on Neural Information Processing   The Conference and Workshop on Neural Information Processing Systems (NIPS) is a machine learning and computational neuroscience conference held every December. The conference is a single track meeting that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts. According to Microsoft Academic Search, NIPS is the top conference on machine learning.  NIPS Deep Learning Workshop   2015,  2014 ,  2013 ,  2012 ,  2011 ,  2010  The Deep Learning and Representation Learning Workshop will be held in conjunction with Neural Information Processing Systems (NIPS)  Deep Learning algorithms attempt to discover good representations, at multiple levels of abstraction. There has been rapid progress in this area in recent years, both in terms of algorithms and in terms of applications, but many challenges remain. The workshop aims at bringing together researchers in that field and discussing these challenges, brainstorming about new solutions.  ACM SIGKDD  2015 ,  2014 ,  2013  KDD provides the premier forum for advancement and adoption of the \"science\" of knowledge discovery and data mining. KDD encourages:  \n     Research in KDD (through annual research conferences, newsletter and other related activities)\nAdoption of \"standards\" in the market in terms of terminology, evaluation, methodology\nInterdisciplinary education among KDD researchers, practitioners, and users\nKDD activities include the annual Conference on Knowledge Discovery and Data Mining and the SIGKDD Explorations Newsletter.", 
            "title": "Conferences"
        }, 
        {
            "location": "/#journals", 
            "text": "JMLR - Journal of Machine Learning Research  The Journal of Machine Learning Research (JMLR) provides an international forum for the electronic and paper publication of high-quality scholarly articles in all areas of machine learning. All published papers are freely available online.  JAIR - Journal of Artificial Intelligence Research  JAIR(ISSN 1076 - 9757) covers all areas of artificial intelligence (AI), publishing refereed research articles, survey articles, and technical notes. Established in 1993 as one of the first electronic scientific journals, JAIR is indexed by INSPEC, Science Citation Index, and MathSciNet. JAIR reviews papers within approximately three months of submission and publishes accepted articles on the internet immediately upon receiving the final versions. JAIR articles are published for free distribution on the internet by the AI Access Foundation, and for purchase in bound volumes by AAAI Press.", 
            "title": "Journals"
        }, 
        {
            "location": "/#groups-and-social-networks", 
            "text": "You can join many peer groups - see  Top 30 LinkedIn Groups for Analytics, Big Data, Data Mining, and Data Science .  AnalyticBridge  is an active community for Analytics and Data Science.  You can attend some of the many  Meetings and Conferences on Analytics, Big Data, Data Mining, Data Science,   Knowledge Discovery .", 
            "title": "Groups and Social Networks"
        }, 
        {
            "location": "/#people", 
            "text": "Jure Leskovec :  #SocialNetworkAnalysis  Andrew Ng :  #DeepLearning  David Blei :  #TopicModel  Matei Zaharia :  #apachespark , #2013,  creator of apache spark, (interview  #1 )", 
            "title": "People"
        }, 
        {
            "location": "/process/", 
            "text": "DS: Process\n\n\nThe good life is a process, not a state of being. It is a direction not a destination.\n\nCarl Rogers\n\n\n\n\n\n\n\nI searched a framework fit for every data mining task, I found a good one from an article of Oracle.\n\n\nAnd here is my summary. The data mining process has 4 steps:\n\n\nStep 1. Problem Definition\n\n\nThis initial phase of a data mining project focuses on understanding the project objectives and requirements. Once you have specified the project from a business perspective, you can formulate it as a data mining problem and develop a preliminary implementation plan.\n\n\nStep 2. Data Gathering \n Preparation\n\n\nThe data understanding phase involves data collection and exploration. As you take a closer look at the data, you can determine how well it addresses the business problem. You might decide to remove some of the data or add additional data. This is also the time to identify data quality problems and to scan for patterns in the data.\n\n\n\n\n\u00a0Data Access\n\n\n\u00a0Data Sampling\n\n\n\u00a0Data Transformation\n\n\n\n\n\nData in the real world is dirty [3]. They are often\u00a0\nincomplete \n(lacking attribute values, lacking certain attributes of\u00a0interest, or containing only aggregate data),\u00a0\nnoisy\n (containing errors or outliers),\u0089 \ninconsistent\n\u00a0(containing discrepancies in codes or names).\n\n\nStep 3. Model Building\n\n\nIn this phase, you select and apply various modeling techniques and calibrate the parameters to optimal values. If the algorithm requires data transformations, you will need to step back to the previous phase to implement them\n\n\n\n\n\u00a0Create Model\n\n\n\n\n\n\u00a0Test Model\n\n\n\u00a0 Evaluate \n Interpret Model\n\n\n\n\nSome important questions [2]:\n\n\n\n    \nIs at least one of predictors useful in predicting the response? (F-statistics)\n\n    \nDo all the predictors help to explain Y, or is only a subset of the predictors useful? (all subsets or best subsets)\n\n    \nHow well does the model fit the data?\n\n    \nGiven a set of predictor values, what response value should we predict, and how accurate is our prediction?\n\n\n\n\n\nStep 4. Knowledge Deployment\n\n\nKnowledge deployment is the use of data mining within a target environment. In the deployment phase, insight and actionable information can be derived from data.\n\n\n\n    \nModel Apply\n\n    \nCustom Reports\n\n    \nExternal Applications\n\n\n\n\n\nReferences\n\n\n\n\n    \nThe Data Mining Process\n, Oracle\n\n    \nTrevor Hastie and Rob Tibshirani, Model Selection and Qualitative Predictors, URL:https://www.youtube.com/watch?v=3T6RXmIHbJ4\n\n    \nNguyen Hung Son, Data cleaning and\u00a0Data preprocessing, URL:http://www.mimuw.edu.pl/~son/datamining/DM/4-preprocess.pdf", 
            "title": "Overview"
        }, 
        {
            "location": "/process/#ds-process", 
            "text": "The good life is a process, not a state of being. It is a direction not a destination. Carl Rogers    I searched a framework fit for every data mining task, I found a good one from an article of Oracle.  And here is my summary. The data mining process has 4 steps:  Step 1. Problem Definition  This initial phase of a data mining project focuses on understanding the project objectives and requirements. Once you have specified the project from a business perspective, you can formulate it as a data mining problem and develop a preliminary implementation plan.  Step 2. Data Gathering   Preparation  The data understanding phase involves data collection and exploration. As you take a closer look at the data, you can determine how well it addresses the business problem. You might decide to remove some of the data or add additional data. This is also the time to identify data quality problems and to scan for patterns in the data.   \u00a0Data Access  \u00a0Data Sampling  \u00a0Data Transformation   Data in the real world is dirty [3]. They are often\u00a0 incomplete  (lacking attribute values, lacking certain attributes of\u00a0interest, or containing only aggregate data),\u00a0 noisy  (containing errors or outliers),\u0089  inconsistent \u00a0(containing discrepancies in codes or names). Step 3. Model Building \n\nIn this phase, you select and apply various modeling techniques and calibrate the parameters to optimal values. If the algorithm requires data transformations, you will need to step back to the previous phase to implement them  \u00a0Create Model   \u00a0Test Model  \u00a0 Evaluate   Interpret Model   Some important questions [2]: \n     Is at least one of predictors useful in predicting the response? (F-statistics) \n     Do all the predictors help to explain Y, or is only a subset of the predictors useful? (all subsets or best subsets) \n     How well does the model fit the data? \n     Given a set of predictor values, what response value should we predict, and how accurate is our prediction?   Step 4. Knowledge Deployment \n\nKnowledge deployment is the use of data mining within a target environment. In the deployment phase, insight and actionable information can be derived from data. \n     Model Apply \n     Custom Reports \n     External Applications   References  \n     The Data Mining Process , Oracle \n     Trevor Hastie and Rob Tibshirani, Model Selection and Qualitative Predictors, URL:https://www.youtube.com/watch?v=3T6RXmIHbJ4 \n     Nguyen Hung Son, Data cleaning and\u00a0Data preprocessing, URL:http://www.mimuw.edu.pl/~son/datamining/DM/4-preprocess.pdf", 
            "title": "DS: Process"
        }, 
        {
            "location": "/problem/", 
            "text": "DS: Problem Definition", 
            "title": "Problem Definitation"
        }, 
        {
            "location": "/problem/#ds-problem-definition", 
            "text": "", 
            "title": "DS: Problem Definition"
        }, 
        {
            "location": "/gathering/", 
            "text": "DS: Gathering \n Preparation\n\n\nDS: Collection\n\n\nOpen Data\n\n\nwikipedia dumps: \nhttps://dumps.wikimedia.org/other/pagecounts-raw/\n\n\nDS: Explore Data Analysis\n\n\nOk. You've assigned a machine learning task, then you get some data. What's next?\n\n\nExplore Data Analysis is step you must do now.\n\n\nQuestion 1: What's size of your data set?\n\n\nHow many rows? How many columns?\n\n\nQuestion 2: What type of variables?\n\n\ncategorical, continuous or discrete\n\n\nQuestion 3: What are missing value?\n\n\nTooling\n\n\n\n\npandas-profiling\n, \ndemo\n\n\n\n\n\n\npip install -U pandas-profiling\n\n\n\n\n[code language=\"python\"]\nimport pandas as pd\nimport pandas_profiling\ndf = pd.read_csv(\"train.csv\", parse_dates=True, encoding='UTF-8')\npandas_profiling.ProfileReport(df)\n[/code]\n\n\nGathering", 
            "title": "Gathering"
        }, 
        {
            "location": "/gathering/#ds-gathering-preparation", 
            "text": "", 
            "title": "DS: Gathering &amp; Preparation"
        }, 
        {
            "location": "/gathering/#ds-collection", 
            "text": "", 
            "title": "DS: Collection"
        }, 
        {
            "location": "/gathering/#open-data", 
            "text": "wikipedia dumps:  https://dumps.wikimedia.org/other/pagecounts-raw/", 
            "title": "Open Data"
        }, 
        {
            "location": "/gathering/#ds-explore-data-analysis", 
            "text": "Ok. You've assigned a machine learning task, then you get some data. What's next?  Explore Data Analysis is step you must do now.", 
            "title": "DS: Explore Data Analysis"
        }, 
        {
            "location": "/gathering/#question-1-whats-size-of-your-data-set", 
            "text": "How many rows? How many columns?", 
            "title": "Question 1: What's size of your data set?"
        }, 
        {
            "location": "/gathering/#question-2-what-type-of-variables", 
            "text": "categorical, continuous or discrete", 
            "title": "Question 2: What type of variables?"
        }, 
        {
            "location": "/gathering/#question-3-what-are-missing-value", 
            "text": "", 
            "title": "Question 3: What are missing value?"
        }, 
        {
            "location": "/gathering/#tooling", 
            "text": "pandas-profiling ,  demo    pip install -U pandas-profiling   [code language=\"python\"]\nimport pandas as pd\nimport pandas_profiling\ndf = pd.read_csv(\"train.csv\", parse_dates=True, encoding='UTF-8')\npandas_profiling.ProfileReport(df)\n[/code]", 
            "title": "Tooling"
        }, 
        {
            "location": "/gathering/#gathering", 
            "text": "", 
            "title": "Gathering"
        }, 
        {
            "location": "/preparation/", 
            "text": "Prepartion\n\n\nNormalization\n\n\nFeature Scaling\n\n\n\n\n\n\nWhy have we do Feature Scaling?\n\n\n\n\nIdea:\n\n\n\nWe have to predict the house prices base on 2 features:\n\n\n    \nHouse sizes (feet\n2\n)\n\n    \nNumber of bedrooms in the house\n\n\n\nAnd we relized that house sizes are about 1000 times \u00a0the number of bedrooms.\u00a0When features diff\u000ber by orders of magnitude,\u00a0first performing feature scaling can make gradient descent converge\u00a0much more quickly.\n\n\n\nGradient Descent with \n without Feature Scaling:\n\n\n\n\n\nThe figure is in the Machine Learning course on Cousera\n\n\nThe left-hand side figure is gradient descent without Feature Scaling and as we seen in this figure, the way of gradient descent to find out the global minimal is way more longer than the right-hand side figure which is gradient descent with Feature scaling\n\n\n\n\nHow to perform Feature Scaling?\n\n\n\n\n\n\n\n    \nSubtract the mean value (the average value) of each feature from the dataset.\n\n    \nAfter subtracting the mean, additionally scale (divide) the feature values\u00a0by their respective \"standard deviations.\"(\nStandard deviations?\n)\n\n    \nFunction:\u00a0\n\u00a0Where \n is the original feature vector, \n is the mean of that feature vector, and \n is its standard deviation.\n\n\n\n\n\n\n\nFeature Scaling Function implementation in Octave:\n\n\nfunction [X_norm, mu, sigma] = featureNormalize(X)\nX_norm = X;\nmu = zeros(1, size(X, 2)); % storing the mean value in mu\nsigma = zeros(1, size(X, 2)); % storing the standard deviation in sigma\n\nfor i = 1:length(mu),\nmu(i) = mean(X(:,i));\nend;\n\nfor i = 1:length(sigma),\nsigma(i) = std(X(:,i));\nend;\n\nX_norm = (X .- mu)./sigma;\nend\n\n\n\n\nSampling", 
            "title": "Preparation"
        }, 
        {
            "location": "/preparation/#prepartion", 
            "text": "", 
            "title": "Prepartion"
        }, 
        {
            "location": "/preparation/#normalization", 
            "text": "", 
            "title": "Normalization"
        }, 
        {
            "location": "/preparation/#feature-scaling", 
            "text": "Why have we do Feature Scaling?", 
            "title": "Feature Scaling"
        }, 
        {
            "location": "/preparation/#feature-scaling-function-implementation-in-octave", 
            "text": "function [X_norm, mu, sigma] = featureNormalize(X)\nX_norm = X;\nmu = zeros(1, size(X, 2)); % storing the mean value in mu\nsigma = zeros(1, size(X, 2)); % storing the standard deviation in sigma\n\nfor i = 1:length(mu),\nmu(i) = mean(X(:,i));\nend;\n\nfor i = 1:length(sigma),\nsigma(i) = std(X(:,i));\nend;\n\nX_norm = (X .- mu)./sigma;\nend", 
            "title": "Feature Scaling Function implementation in Octave:"
        }, 
        {
            "location": "/preparation/#sampling", 
            "text": "", 
            "title": "Sampling"
        }, 
        {
            "location": "/models/", 
            "text": "DS: Model Building\n\n\nHow to learn a ML Algorithm\n\n\n1.\n Motivation\n\n\nEach algorithm have its own motivation. It may a simple example to see how it work\n\n\n2.\n Problem Definition\n\n\nWhere can we apply this algorithm? How did it work in real world applications\n\n\n3.\n Mathematics Representation\n\n\nProblem Equations, notations\n\n\nWe will discuss about mathematics representation of algorithm, notations we use for problem\n\n\n4.\n Algorithm\n\n\nWe will discuss how to solve this mathematics problems\n\n\n5.\n Examples\n\n\nWe will apply algorithm with a few examples (1-2 dimension is highly recommended, because we will plot these data and model easily)\n\n\nIn this section, we can see how well (bad) algorithm works with these data\n\n\n6.\n Implementation Notice\n\n\nWe will give some notes about implement this algorithm to real world problems. What case we want to apply this algorithm? What case we don't?\n\n\n7.\n Quiz\n\n\nOne way to rethink about problem is doing quiz.\n\n\n8.\n Exercise\n\n\nNow we give readers opportunity to work will this algorithm by themselves.\n\n\nMicrosoft Azure Machine Learning \n1\n\n\n\n\nMachine Learning Cheat Sheet for scikit-learn \n2\n\n\n\n\nDLib C++ Library - Machine Learning Guide \n3\n\n\n\n\nChallenges\n\n\n\n\nVery much features (\n 100)\n\n\nVery much data (\n 1e9 items)\n\n\nText Data, Images, Videos\n\n\nTraining Times\n\n\nAccuracy, Over Fitting\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning algorithm cheat sheet for Microsoft Azure Machine Learning Studio\n\n\n\n\n\n\nMachine Learning Cheat Sheet (for scikit-learn)\n\n\n\n\n\n\nDLib C++ Library - Machine Learning Guide", 
            "title": "Models"
        }, 
        {
            "location": "/models/#ds-model-building", 
            "text": "", 
            "title": "DS: Model Building"
        }, 
        {
            "location": "/models/#how-to-learn-a-ml-algorithm", 
            "text": "1.  Motivation  Each algorithm have its own motivation. It may a simple example to see how it work  2.  Problem Definition  Where can we apply this algorithm? How did it work in real world applications  3.  Mathematics Representation  Problem Equations, notations  We will discuss about mathematics representation of algorithm, notations we use for problem  4.  Algorithm  We will discuss how to solve this mathematics problems  5.  Examples  We will apply algorithm with a few examples (1-2 dimension is highly recommended, because we will plot these data and model easily)  In this section, we can see how well (bad) algorithm works with these data  6.  Implementation Notice  We will give some notes about implement this algorithm to real world problems. What case we want to apply this algorithm? What case we don't?  7.  Quiz  One way to rethink about problem is doing quiz.  8.  Exercise  Now we give readers opportunity to work will this algorithm by themselves.", 
            "title": "How to learn a ML Algorithm"
        }, 
        {
            "location": "/models/#microsoft-azure-machine-learning-1", 
            "text": "", 
            "title": "Microsoft Azure Machine Learning 1"
        }, 
        {
            "location": "/models/#machine-learning-cheat-sheet-for-scikit-learn-2", 
            "text": "", 
            "title": "Machine Learning Cheat Sheet for scikit-learn 2"
        }, 
        {
            "location": "/models/#dlib-c-library-machine-learning-guide-3", 
            "text": "", 
            "title": "DLib C++ Library - Machine Learning Guide 3"
        }, 
        {
            "location": "/models/#challenges", 
            "text": "Very much features (  100)  Very much data (  1e9 items)  Text Data, Images, Videos  Training Times  Accuracy, Over Fitting       Machine learning algorithm cheat sheet for Microsoft Azure Machine Learning Studio    Machine Learning Cheat Sheet (for scikit-learn)    DLib C++ Library - Machine Learning Guide", 
            "title": "Challenges"
        }, 
        {
            "location": "/model_regression/", 
            "text": "Regression\n\n\nLinear Regression\n\n\nIn-Out\n\n\n\n\n\nInput: \nContinuous\n\nOutput: \nContinuous\n\n\nWhen to use \n1\n\n\n\n\nEconometric Modeling\n\n\nMarketing Mix Model\n\n\nCustomer Lifetime Value\n\n\n\n\n\nExamples\n\n\nEx1. Linear Regression with Boston Dataset\n\n\n__author__ = 'rain'\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge\nboston = load_boston()\ndata = boston['data']\nX, y = data[:, :-1], data[:, -1]\nX_train, X_test, y_train, y_test =\n  train_test_split(X, y, test_size=0.3)\nprint boston['DESCR']\nclf_linear = LinearRegression()\nclf_linear.fit(X_train, y_train)\nlinear_score = clf_linear.score(X_test, y_test)\n#-\n 0.671\nprint(clf_linear.coef_)\nprint(clf_linear.intercept_)\n\nclf_ridge = Ridge(alpha=1.0)\nclf_ridge.fit(X_train, y_train)\n# 0.674\nridge_score = clf_ridge.score(X_test, y_test)\n\nprint y_test\nprint clf_linear.predict(X_test)\nprint clf_ridge.predict(X_test)\n\n\n\n\nEx2. Linear Regression with market data set (coursera)\n ([python language=\"notebook\"]\n/python\n)\n\n\nLogistic Regression\n\n\n\n\nIn-Out \n1\n\n\n\n\n\nIn: \ncontinuos\n\n\nOut: \nTrue/False\n\n\n\n\n\n1. Hyposthesis Representation\n\n\n\n\nh_{\\theta}(x) = g(\\theta^T x)\\ where\\ g(z) = \\frac{1}{1 + e^{-z}}\n\n\n\n\n\n\n g(z) \n is sigmoid function or logistic function\n\n\n\n\n h_{\\theta}(x) \n estimated probability of \n y = 1\n given \n x \n\n\n\n\nIn spam detection problem, \n h_{\\theta}(x) = 0.7 \n means it's 70% chance this email is spam.\n\n\n2. Decision Boundary\n\n\nLogistic Regression\n\n\n3. Cost Function\n\n\n\n\ncost(h_{\\theta}(x), y) = -y log(h_{\\theta}(x)) - (1-y) log(1- h_{\\theta}(x))\n\n\n\n\nLoss Function\n\n\n\n\nJ(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} cost(h_{\\theta}(x^{(i)}), y^{(i)}) = \\frac{-1}{m} \\sum_{i=1}^{m} y^{(i)} log h_{\\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))\n\n\n\n\n4. Gradient Descent\n\n\nGradient\n\n\n\n\n\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum^m_{i=1} (h_\\theta (x^{(i)}) - y^{(i)}) x_j^{(i)}\n\n\n\n\n5. Predict\n\n\n\n\np(\\theta, X) = h_\\theta(X) \\ge 0.5\n\n\n\n\n6. Regularization\n\n\n6.1 Feature Mapping\n\n\nCost Function\n\n\n\n\nmapFeature(x) =\n\\begin{bmatrix}\n1 \\\\\nx_1 \\\\\nx_2 \\\\\nx_1^2 \\\\\nx_1 x_2 \\\\\nx_2^2 \\\\\nx_1^3 \\\\\n\\cdots \\\\\nx_1 x_2^5\\\\\nx_2^6\n\\end{bmatrix}\n\n\n\n\n\n6.2 Cost Function and Gradient\n\n\nCost Function\n\n\nJ(\\theta) = \\frac{1}{m} \\sum^m_{i=1} [-y^{(i)} log(h_\\theta(x^{(i)})) - (1 - y^{(i)})log(1 - h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum^n_{j=1} \\theta^2_j\n\n\n\n\n\nGradient\n\n\n\n\n \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum^m_{i=1}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \n\nfor \n j = 0 \n\n\n\n\n\n\n \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum^m_{i=1}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_j \n for \n j \\ge 1 \n\n\n\n\nCode\n\n\n\nBank Marketing Data Set\n\n\nimport statsmodels.api as sm\nimport pandas as pd\nfrom statsmodels.tools.tools import categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport numpy\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndef get_data():\n    return pd.read_csv(\nquot;./bank/bank-full.csv\nquot;, header=0, sep=\nquot;;\nquot;)\n\ndata = get_data()\n\ndata.job = LabelEncoder().fit_transform(data.job)\ndata.marital = LabelEncoder().fit_transform(data.marital)\ndata.education = LabelEncoder().fit_transform(data.education)\ndata.default = LabelEncoder().fit_transform(data.default)\ndata.housing = LabelEncoder().fit_transform(data.housing)\ndata.loan = LabelEncoder().fit_transform(data.loan)\ndata.month = LabelEncoder().fit_transform(data.month)\ndata.contact = LabelEncoder().fit_transform(data.contact)\ndata.poutcome = LabelEncoder().fit_transform(data.poutcome)\n\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\nscore = clf.score(X_test, y_test)\n\nprint confusion_matrix(y_test, clf.predict(X_test))\n# [[11807   203]\n#  [ 1243   311]]\n# it's too bad\n\n\n\n\n\nExamples\n\n\n\n\nAffair Dataset, Logistic Regression with scikit-learn\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression vs Logistic Regression vs Poisson Regression", 
            "title": "Regression"
        }, 
        {
            "location": "/model_regression/#regression", 
            "text": "", 
            "title": "Regression"
        }, 
        {
            "location": "/model_regression/#linear-regression", 
            "text": "", 
            "title": "Linear Regression"
        }, 
        {
            "location": "/model_regression/#when-to-use-1", 
            "text": "Econometric Modeling  Marketing Mix Model  Customer Lifetime Value", 
            "title": "When to use 1"
        }, 
        {
            "location": "/model_regression/#examples", 
            "text": "Ex1. Linear Regression with Boston Dataset  __author__ = 'rain'\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge\nboston = load_boston()\ndata = boston['data']\nX, y = data[:, :-1], data[:, -1]\nX_train, X_test, y_train, y_test =\n  train_test_split(X, y, test_size=0.3)\nprint boston['DESCR']\nclf_linear = LinearRegression()\nclf_linear.fit(X_train, y_train)\nlinear_score = clf_linear.score(X_test, y_test)\n#-  0.671\nprint(clf_linear.coef_)\nprint(clf_linear.intercept_)\n\nclf_ridge = Ridge(alpha=1.0)\nclf_ridge.fit(X_train, y_train)\n# 0.674\nridge_score = clf_ridge.score(X_test, y_test)\n\nprint y_test\nprint clf_linear.predict(X_test)\nprint clf_ridge.predict(X_test)  Ex2. Linear Regression with market data set (coursera)  ([python language=\"notebook\"] /python )", 
            "title": "Examples"
        }, 
        {
            "location": "/model_regression/#logistic-regression", 
            "text": "", 
            "title": "Logistic Regression"
        }, 
        {
            "location": "/model_regression/#1-hyposthesis-representation", 
            "text": "h_{\\theta}(x) = g(\\theta^T x)\\ where\\ g(z) = \\frac{1}{1 + e^{-z}}     g(z)   is sigmoid function or logistic function    h_{\\theta}(x)   estimated probability of   y = 1  given   x    In spam detection problem,   h_{\\theta}(x) = 0.7   means it's 70% chance this email is spam.", 
            "title": "1. Hyposthesis Representation"
        }, 
        {
            "location": "/model_regression/#2-decision-boundary", 
            "text": "Logistic Regression", 
            "title": "2. Decision Boundary"
        }, 
        {
            "location": "/model_regression/#3-cost-function", 
            "text": "cost(h_{\\theta}(x), y) = -y log(h_{\\theta}(x)) - (1-y) log(1- h_{\\theta}(x))   Loss Function   J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} cost(h_{\\theta}(x^{(i)}), y^{(i)}) = \\frac{-1}{m} \\sum_{i=1}^{m} y^{(i)} log h_{\\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))", 
            "title": "3. Cost Function"
        }, 
        {
            "location": "/model_regression/#4-gradient-descent", 
            "text": "Gradient   \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum^m_{i=1} (h_\\theta (x^{(i)}) - y^{(i)}) x_j^{(i)}", 
            "title": "4. Gradient Descent"
        }, 
        {
            "location": "/model_regression/#5-predict", 
            "text": "p(\\theta, X) = h_\\theta(X) \\ge 0.5", 
            "title": "5. Predict"
        }, 
        {
            "location": "/model_regression/#6-regularization", 
            "text": "", 
            "title": "6. Regularization"
        }, 
        {
            "location": "/model_regression/#61-feature-mapping", 
            "text": "Cost Function   mapFeature(x) =\n\\begin{bmatrix}\n1 \\\\\nx_1 \\\\\nx_2 \\\\\nx_1^2 \\\\\nx_1 x_2 \\\\\nx_2^2 \\\\\nx_1^3 \\\\\n\\cdots \\\\\nx_1 x_2^5\\\\\nx_2^6\n\\end{bmatrix}", 
            "title": "6.1 Feature Mapping"
        }, 
        {
            "location": "/model_regression/#62-cost-function-and-gradient", 
            "text": "Cost Function \nJ(\\theta) = \\frac{1}{m} \\sum^m_{i=1} [-y^{(i)} log(h_\\theta(x^{(i)})) - (1 - y^{(i)})log(1 - h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum^n_{j=1} \\theta^2_j   Gradient    \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum^m_{i=1}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}  \nfor   j = 0      \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum^m_{i=1}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_j   for   j \\ge 1", 
            "title": "6.2 Cost Function and Gradient"
        }, 
        {
            "location": "/model_regression/#examples_1", 
            "text": "Affair Dataset, Logistic Regression with scikit-learn       Linear Regression vs Logistic Regression vs Poisson Regression", 
            "title": "Examples"
        }, 
        {
            "location": "/model_classification/", 
            "text": "Classification\n\n\n\n\nClassification \n1\n\n\nA very familiar example is the email spam-catching system: given a set of emails marked as spam and not-spam, it learns the characteristics of spam emails and is then able to process future email messages to mark them as spam or not-spam.\n\n\nThe technique used in the above example of email spam-catching system is one of the most common machine learning techniques: classification (actually, statistical classification). More precisely it is a supervised statistical classification. Supervised because the system needs to be first trained using already classified training data as opposed to an unsupervised system where such training is not done.\n\n\nA supervised learning system that performs classification is known as a learner or, more commonly, a classifier.\n\n\nThe classifier is first fed training data in which each item is already labeled with the correct label or class. This data is used to train the learning algorithm, which creates models that can then be used to label/classify similar data.\n\n\nFormally, given a set of input items, \n and a set of labels/classes, \n and training data \nis the label/class for $latex x_i$, a classifier is a mapping from X to Y $latex f(T, x) = y$.\n\n\nBinary Classification\n\n\nAlgorithms \n1\n\n\n\n\nTwo-class SVM\n\n\n\n\n\n\n100 features, linear model\n\n\n\n\n\n\nTwo-class Logistic Regression\n\n\nFast training, linear model\n\n\nTwo-class Bayes point machine\n\n\nFast training, linear model\n\n\nTwo-class random forest\n\n\nAccuracy, fast training\n\n\nTwo-class boosted decision tree\n\n\nAccuracy, fast training\n\n\nTwo-class neural network\n\n\nAccuracy, long training times\n\n\n\n\nMulticlass Classification\n\n\n\n\nIntroduction \n2\n\n\nIn machine learning, multiclass or multinomial classification is the problem of classifying instances into one of the more than two classes (classifying instances into one of the two classes is called binary classification).\n\n\nWhile some classification algorithms naturally permit the use of more than two classes, others are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies.\n\n\nMulticlass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.\n\n\nAlgorithms \n1\n\n\n\n\nMulticlass Logistic Regression\n\n\nMulticlass SVM\n\n\nMulticlass Neural Network\n\n\nMulticlass Decision Forest\n\n\nMulticlass Decision Jungle\n\n\n\n\nConfusion Matrix\n\n\nsklearn plot confusion matrix with labels \n3\n\n\nimport matplotlib.pyplot as plt\ndef plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=None):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(cm)\n    plt.title(title)\n    fig.colorbar(cax)\n    if labels:\n        ax.set_xticklabels([''] + labels)\n        ax.set_yticklabels([''] + labels)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n\n\n\n\n\n\nMultilabel Classification\n\n\n\n\nIntroduction \n1\n\n\nIn machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple target labels must be assigned to each instance. Multi-label classification should not be confused with multiclass classification, which is the problem of categorizing instances into one of more than two classes. Formally, multi-label learning can be phrased as the problem of finding a model that maps inputs x to binary vectors y, rather than scalar outputs as in the ordinary classification problem.\n\n\nThere are two main methods for tackling the multi-label classification problem:[1] problem transformation methods and algorithm adaptation methods. Problem transformation methods transform the multi-label problem into a set of binary classification problems, which can then be handled using single-class classifiers. Algorithm adaptation methods adapt the algorithms to directly perform multi-label classification. In other words, rather than trying to convert the problem to a simpler problem, they try to address the problem in its full form.\n\n\nImplements\n\n\n\n\nMulticlass and multilabel algorithms\n\n\n\n\nSVM\n\n\n\n\n\n\n\n\n\n\nMulti-label classification\n\n\n\n\n\n\nMulticlass classification\n\n\n\n\n\n\nsklearn plot confusion matrix with labels", 
            "title": "Classification"
        }, 
        {
            "location": "/model_classification/#classification", 
            "text": "", 
            "title": "Classification"
        }, 
        {
            "location": "/model_classification/#classification-1", 
            "text": "A very familiar example is the email spam-catching system: given a set of emails marked as spam and not-spam, it learns the characteristics of spam emails and is then able to process future email messages to mark them as spam or not-spam.  The technique used in the above example of email spam-catching system is one of the most common machine learning techniques: classification (actually, statistical classification). More precisely it is a supervised statistical classification. Supervised because the system needs to be first trained using already classified training data as opposed to an unsupervised system where such training is not done.  A supervised learning system that performs classification is known as a learner or, more commonly, a classifier.  The classifier is first fed training data in which each item is already labeled with the correct label or class. This data is used to train the learning algorithm, which creates models that can then be used to label/classify similar data.  Formally, given a set of input items,   and a set of labels/classes,   and training data  is the label/class for $latex x_i$, a classifier is a mapping from X to Y $latex f(T, x) = y$.", 
            "title": "Classification 1"
        }, 
        {
            "location": "/model_classification/#binary-classification", 
            "text": "", 
            "title": "Binary Classification"
        }, 
        {
            "location": "/model_classification/#algorithms-1", 
            "text": "Two-class SVM    100 features, linear model    Two-class Logistic Regression  Fast training, linear model  Two-class Bayes point machine  Fast training, linear model  Two-class random forest  Accuracy, fast training  Two-class boosted decision tree  Accuracy, fast training  Two-class neural network  Accuracy, long training times", 
            "title": "Algorithms 1"
        }, 
        {
            "location": "/model_classification/#multiclass-classification", 
            "text": "", 
            "title": "Multiclass Classification"
        }, 
        {
            "location": "/model_classification/#introduction-2", 
            "text": "In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of the more than two classes (classifying instances into one of the two classes is called binary classification).  While some classification algorithms naturally permit the use of more than two classes, others are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies.  Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.", 
            "title": "Introduction 2"
        }, 
        {
            "location": "/model_classification/#algorithms-1_1", 
            "text": "Multiclass Logistic Regression  Multiclass SVM  Multiclass Neural Network  Multiclass Decision Forest  Multiclass Decision Jungle", 
            "title": "Algorithms 1"
        }, 
        {
            "location": "/model_classification/#confusion-matrix", 
            "text": "sklearn plot confusion matrix with labels  3  import matplotlib.pyplot as plt\ndef plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=None):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(cm)\n    plt.title(title)\n    fig.colorbar(cax)\n    if labels:\n        ax.set_xticklabels([''] + labels)\n        ax.set_yticklabels([''] + labels)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()", 
            "title": "Confusion Matrix"
        }, 
        {
            "location": "/model_classification/#multilabel-classification", 
            "text": "", 
            "title": "Multilabel Classification"
        }, 
        {
            "location": "/model_classification/#introduction-1", 
            "text": "In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple target labels must be assigned to each instance. Multi-label classification should not be confused with multiclass classification, which is the problem of categorizing instances into one of more than two classes. Formally, multi-label learning can be phrased as the problem of finding a model that maps inputs x to binary vectors y, rather than scalar outputs as in the ordinary classification problem.  There are two main methods for tackling the multi-label classification problem:[1] problem transformation methods and algorithm adaptation methods. Problem transformation methods transform the multi-label problem into a set of binary classification problems, which can then be handled using single-class classifiers. Algorithm adaptation methods adapt the algorithms to directly perform multi-label classification. In other words, rather than trying to convert the problem to a simpler problem, they try to address the problem in its full form.", 
            "title": "Introduction 1"
        }, 
        {
            "location": "/model_classification/#implements", 
            "text": "Multiclass and multilabel algorithms", 
            "title": "Implements"
        }, 
        {
            "location": "/model_classification/#svm", 
            "text": "Multi-label classification    Multiclass classification    sklearn plot confusion matrix with labels", 
            "title": "SVM"
        }, 
        {
            "location": "/model_clustering/", 
            "text": "Clustering\n\n\nUsing K-Means to cluster wine dataset\n\n\nRecently, I joined\u00a0\nCluster Analysis\n\u00a0course in coursera. The content of first week is about Partitioning-Based Clustering Methods where I learned about some cluster algorithms based on distance such as\u00a0\nK-Means\n,\u00a0\nK-Medians\n\u00a0and\u00a0\nK-Modes\n. I would like to turn what I learn into practice so I write this post as an excercise of this course.\n\n\n\nIn this post, I will use K-Means for clustering\u00a0\nwine\n\u00a0data set which I found in one of excellent posts about K-Mean in\u00a0\nr-statistics website\n.\n\n\n\nMeet the data\n\n\n\n\n\n\nThe\u00a0\nwine\n\u00a0data set contains the results of a chemical analysis of wines grown in a specific area of Italy. Three types of wine are represented in the 178 samples, with the results of 13 chemical analyses recorded for each sample. The Type variable has been transformed into a categoric variable.\n\n\n\n data(wine, package=\nquot;rattle\nquot;)\nhead(wine)\n\n#\ngt;   Type Alcohol Malic  Ash Alcalinity Magnesium Phenols\n#\ngt; 1    1   14.23  1.71 2.43       15.6       127    2.80\n#\ngt; 2    1   13.20  1.78 2.14       11.2       100    2.65\n#\ngt; 3    1   13.16  2.36 2.67       18.6       101    2.80\n#\ngt; 4    1   14.37  1.95 2.50       16.8       113    3.85\n#\ngt; 5    1   13.24  2.59 2.87       21.0       118    2.80\n#\ngt; 6    1   14.20  1.76 2.45       15.2       112    3.27\n#\ngt;   Flavanoids Nonflavanoids Proanthocyanins Color  Hue\n#\ngt; 1       3.06          0.28            2.29  5.64 1.04\n#\ngt; 2       2.76          0.26            1.28  4.38 1.05\n#\ngt; 3       3.24          0.30            2.81  5.68 1.03\n#\ngt; 4       3.49          0.24            2.18  7.80 0.86\n#\ngt; 5       2.69          0.39            1.82  4.32 1.04\n#\ngt; 6       3.39          0.34            1.97  6.75 1.05\n#\ngt;   Dilution Proline\n#\ngt; 1     3.92    1065\n#\ngt; 2     3.40    1050\n#\ngt; 3     3.17    1185\n#\ngt; 4     3.45    1480\n#\ngt; 5     2.93     735\n#\ngt; 6     2.85    1450\n\n\n\n\nExplore and Preprocessing Data\n\n\n\nLet\ns see structure of wine data set\n\n\n\n str(wine)\n\n#\ngt; \napos;data.frame\napos;:  178 obs. of  14 variables:\n#\ngt; $ Type           : Factor w/ 3 levels \nquot;1\nquot;,\nquot;2\nquot;,\nquot;3\nquot;: 1 1 1 1 1 1 1 1 1 1 ...\n#\ngt; $ Alcohol        : num  14.2 13.2 13.2 14.4 13.2 ...\n#\ngt; $ Malic          : num  1.71 1.78 2.36 1.95 2.59 1.76 1.87 2.15 1.64 1.35 ...\n#\ngt; $ Ash            : num  2.43 2.14 2.67 2.5 2.87 2.45 2.45 2.61 2.17 2.27 ...\n#\ngt; $ Alcalinity     : num  15.6 11.2 18.6 16.8 21 15.2 14.6 17.6 14 16 ...\n#\ngt; $ Magnesium      : int  127 100 101 113 118 112 96 121 97 98 ...\n#\ngt; $ Phenols        : num  2.8 2.65 2.8 3.85 2.8 3.27 2.5 2.6 2.8 2.98 ...\n#\ngt; $ Flavanoids     : num  3.06 2.76 3.24 3.49 2.69 3.39 2.52 2.51 2.98 3.15 ...\n#\ngt; $ Nonflavanoids  : num  0.28 0.26 0.3 0.24 0.39 0.34 0.3 0.31 0.29 0.22 ...\n#\ngt; $ Proanthocyanins: num  2.29 1.28 2.81 2.18 1.82 1.97 1.98 1.25 1.98 1.85 ...\n#\ngt; $ Color          : num  5.64 4.38 5.68 7.8 4.32 6.75 5.25 5.05 5.2 7.22 ...\n#\ngt; $ Hue            : num  1.04 1.05 1.03 0.86 1.04 1.05 1.02 1.06 1.08 1.01 ...\n#\ngt; $ Dilution       : num  3.92 3.4 3.17 3.45 2.93 2.85 3.58 3.58 2.85 3.55 ...\n#\ngt; $ Proline        : int  1065 1050 1185 1480 735 1450 1290 1295 1045 1045 ...\n\n\n\n\nWine data set contains 1 categorical variables (label) and 13 numerical variables. But these numerical variables is not scaled, I use\u00a0\nscale\n\u00a0function for scaling and centering data and then assign it as training data.\n\n\n\n data.train \nlt;- scale(wine[-1])\n\n\n\n\nData is already centered and scaled.\n\n\n\n summary(data.train)\n#\ngt;   Alcohol             Malic\n#\ngt; Min.   :-2.42739   Min.   :-1.4290\n#\ngt; 1st Qu.:-0.78603   1st Qu.:-0.6569\n#\ngt; Median : 0.06083   Median :-0.4219\n#\ngt; Mean   : 0.00000   Mean   : 0.0000\n#\ngt; 3rd Qu.: 0.83378   3rd Qu.: 0.6679\n#\ngt; Max.   : 2.25341   Max.   : 3.1004\n#\ngt;      Ash             Alcalinity\n#\ngt; Min.   :-3.66881   Min.   :-2.663505\n#\ngt; 1st Qu.:-0.57051   1st Qu.:-0.687199\n#\ngt; Median :-0.02375   Median : 0.001514\n#\ngt; Mean   : 0.00000   Mean   : 0.000000\n#\ngt; 3rd Qu.: 0.69615   3rd Qu.: 0.600395\n#\ngt; Max.   : 3.14745   Max.   : 3.145637\n#\ngt;   Magnesium          Phenols\n#\ngt; Min.   :-2.0824   Min.   :-2.10132\n#\ngt; 1st Qu.:-0.8221   1st Qu.:-0.88298\n#\ngt; Median :-0.1219   Median : 0.09569\n#\ngt; Mean   : 0.0000   Mean   : 0.00000\n#\ngt; 3rd Qu.: 0.5082   3rd Qu.: 0.80672\n#\ngt; Max.   : 4.3591   Max.   : 2.53237\n#\ngt;   Flavanoids      Nonflavanoids\n#\ngt; Min.   :-1.6912   Min.   :-1.8630\n#\ngt; 1st Qu.:-0.8252   1st Qu.:-0.7381\n#\ngt; Median : 0.1059   Median :-0.1756\n#\ngt; Mean   : 0.0000   Mean   : 0.0000\n#\ngt; 3rd Qu.: 0.8467   3rd Qu.: 0.6078\n#\ngt; Max.   : 3.0542   Max.   : 2.3956\n#\ngt; Proanthocyanins        Color\n#\ngt; Min.   :-2.06321   Min.   :-1.6297\n#\ngt; 1st Qu.:-0.59560   1st Qu.:-0.7929\n#\ngt; Median :-0.06272   Median :-0.1588\n#\ngt; Mean   : 0.00000   Mean   : 0.0000\n#\ngt; 3rd Qu.: 0.62741   3rd Qu.: 0.4926\n#\ngt; Max.   : 3.47527   Max.   : 3.4258\n#\ngt;      Hue              Dilution\n#\ngt; Min.   :-2.08884   Min.   :-1.8897\n#\ngt; 1st Qu.:-0.76540   1st Qu.:-0.9496\n#\ngt; Median : 0.03303   Median : 0.2371\n#\ngt; Mean   : 0.00000   Mean   : 0.0000\n#\ngt; 3rd Qu.: 0.71116   3rd Qu.: 0.7864\n#\ngt; Max.   : 3.29241   Max.   : 1.9554\n#\ngt;    Proline\n#\ngt; Min.   :-1.4890\n#\ngt; 1st Qu.:-0.7824\n#\ngt; Median :-0.2331\n#\ngt; Mean   : 0.0000\n#\ngt; 3rd Qu.: 0.7561\n# \ngt; Max.   : 2.963\n\n\n\n\nModel Fitting\n\n\n\nNow the fun part begins. I use\u00a0\nNbClust\n\u00a0function to determine what is the best number of clusteres\u00a0\nk\n\u00a0for K-Means\n\n\n\n nc \nlt;- NbClust(data.train,\n              min.nc=2, max.nc=15,\n              method=\nquot;kmeans\nquot;)\nbarplot(table(nc$Best.n[1,]),\n        xlab=\nquot;Numer of Clusters\nquot;,\n        ylab=\nquot;Number of Criteria\nquot;,\n        main=\nquot;Number of Clusters Chosen by 26 Criteria\nquot;)\n\n\n\n\n\n\n\nAccording to the graph, we can find the best number of clusters is 3. Beside\u00a0\nNbClust\n\u00a0function which provides 30 indices for determing the number of clusters and proposes the best clustering scheme, we can draw the sum of square error (SSE) scree plot and look for a bend or elbow in this graph to determine appropriate k\n\n\n\n wss \nlt;- 0\nfor (i in 1:15){\n  wss[i] \nlt;-\n    sum(kmeans(data.train, centers=i)$withinss)\n}\nplot(1:15,\n  wss,\n  type=\nquot;b\nquot;,\n  xlab=\nquot;Number of Clusters\nquot;,\n  ylab=\nquot;Within groups sum of squares\nquot;)\n\n\n\n\n\n\n\nBoth two methods suggest k=3 is best choice for us. It\ns reasonsable if we take notice that the original data set also contains 3 classes.\n\n\n\nFit the model\n\n\n\nWe now fit\u00a0\nwine\n\u00a0data to K-Means with k = 3\n\n\n\n fit.km \nlt;- kmeans(data.train, 3)\n\n\n\n\nThen interpret the result\n\n\n\n fit.km\n\n#\ngt; K-means clustering with 3 clusters of sizes 51, 65, 62\n#\ngt;\n#\ngt; Cluster means:\n#\ngt;      Alcohol      Malic        Ash Alcalinity\n#\ngt; 1  0.1644436  0.8690954  0.1863726  0.5228924\n#\ngt; 2 -0.9234669 -0.3929331 -0.4931257  0.1701220\n#\ngt; 3  0.8328826 -0.3029551  0.3636801 -0.6084749\n#\ngt;     Magnesium     Phenols  Flavanoids Nonflavanoids\n#\ngt; 1 -0.07526047 -0.97657548 -1.21182921    0.72402116\n#\ngt; 2 -0.49032869 -0.07576891  0.02075402   -0.03343924\n#\ngt; 3  0.57596208  0.88274724  0.97506900   -0.56050853\n#\ngt;   Proanthocyanins      Color        Hue   Dilution\n#\ngt; 1     -0.77751312  0.9388902 -1.1615122 -1.2887761\n#\ngt; 2      0.05810161 -0.8993770  0.4605046  0.2700025\n#\ngt; 3      0.57865427  0.1705823  0.4726504  0.7770551\n#\ngt;      Proline\n#\ngt; 1 -0.4059428\n#\ngt; 2 -0.7517257\n#\ngt; 3  1.1220202\n#\ngt;\n#\ngt; Clustering vector:\n#\ngt;   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#\ngt;  [26] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#\ngt;  [51] 3 3 3 3 3 3 3 3 3 2 2 1 2 2 2 2 2 2 2 2 2 2 2 3 2\n#\ngt;  [76] 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2\n#\ngt; [101] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 3 2 2 2\n#\ngt; [126] 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#\ngt; [151] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#\ngt; [176] 1 1 1\n#\ngt;\n#\ngt; Within cluster sum of squares by cluster:\n#\ngt; [1] 326.3537 558.6971 385.6983\n#\ngt;  (between_SS / total_SS =  44.8 %)\n#\ngt;\n#\ngt; Available components:\n#\ngt;\n#\ngt; [1] \nquot;cluster\nquot;      \nquot;centers\nquot;      \nquot;totss\nquot;\n#\ngt; [4] \nquot;withinss\nquot;     \nquot;tot.withinss\nquot; \nquot;betweenss\nquot;\n# \ngt; [7] \nquot;size\nquot;         \nquot;iter\nquot;         \nquot;ifault\nquot\n\n\n\n\nThe result shows information about cluster means, clustering vector, sum of square by cluster and available components. Let\ns do some visualizations to see how data set is clustered.\n\n\n\nFirst, I use\u00a0\nplotcluster\n\u00a0function from\u00a0\nfpc\n\u00a0package to draw discriminant projection plot\n\n\n\n library(fpc)\nplotcluster(data.train, fit.km$cluster)\n\n\n\n\n\n\n\nWe can see the data is clustered very well, there are no collapse between clusters. Next, we draw parallel coordinates plot to see how variables contributed in each cluster\n\n\n\n library(MASS)\nparcoord(data.train, fit.km$cluster)\n\n\n\n\n\n\n\nWe can extract some insights from above graph suc as black cluster contains wine with low flavanoids value, low proanthocyanins value, low hue value. Or green cluster contains wine which has dilution value higher than wine in red cluster.\n\n\n\nEvaluation\n\n\n\nBecause the original data set\u00a0\nwine\n\u00a0also has 3 classes, it is reasonable if we compare these classes with 3 clusters fited by K-Means\n\n\n\n confuseTable.km \nlt;- table(wine$Type, fit.km$cluster)\nconfuseTable.km\n#\ngt;    1  2  3\n#\ngt; 1  0  0 59\n#\ngt; 2  3 65  3\n# \ngt; 3 48  0  \n\n\n\n\nWe can see only 6 sample is missed. Let\ns use randIndex from flexclust to compare these two parititions - one from data set and one from result of clustering method.\n\n\n\n library(flexclust)\nrandIndex(ct.km)\n#\ngt;      ARI\n#\ngt; 0.897495\n\n\n\n\nIt\ns quite close to 1 so K-Means is good model for clustering\u00a0\nwine\n\u00a0data set.\n\n\n\nReferences\n\n\n\nChoosing number of cluster in K-Means,\u00a0\nhttp://stackoverflow.com/a/15376462/1036500\nK-means Clustering (from \u201cR in Action\u201d),\u00a0\nhttp://www.r-statistics.com/2013/08/k-means-clustering-from-r-in-action/\nColor the cluster output in r, \u00a0\nhttp://stackoverflow.com/questions/15386960/color-the-cluster-output-in-r", 
            "title": "Clustering"
        }, 
        {
            "location": "/model_clustering/#clustering", 
            "text": "", 
            "title": "Clustering"
        }, 
        {
            "location": "/model_clustering/#using-k-means-to-cluster-wine-dataset", 
            "text": "Recently, I joined\u00a0 Cluster Analysis \u00a0course in coursera. The content of first week is about Partitioning-Based Clustering Methods where I learned about some cluster algorithms based on distance such as\u00a0 K-Means ,\u00a0 K-Medians \u00a0and\u00a0 K-Modes . I would like to turn what I learn into practice so I write this post as an excercise of this course.  In this post, I will use K-Means for clustering\u00a0 wine \u00a0data set which I found in one of excellent posts about K-Mean in\u00a0 r-statistics website .", 
            "title": "Using K-Means to cluster wine dataset"
        }, 
        {
            "location": "/ensemble/", 
            "text": "Ensemble\n\n\n\n\nEnsemble Algorithms \n1\n\n\nEnsemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction.\n\n\nMuch effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.\n\n\n\n\nBoosting\n\n\nBootstrapped Aggregation (Bagging)\n\n\nAdaBoost\n\n\nStacked Generalization (blending)\n\n\nGradient Boosting Machines (GBM)\n\n\nGradient Boosted Regression Trees (GBRT)\n\n\nRandom Forest\n\n\n\n\nXGBoost\n\n\n\n\nXGBoost is short for eXtreme gradient boosting.\n\n\n\n\nFeatures \n1\n\n\n\n\nEasy to use\n\n\n\n\nEasy to install\n\n\nHighly developed R/python for users\n\n\n\n\nEfficiency\n\n\n\n\nAutomatic parallel computation on a single machine\n\n\nCan be run on a cluster.\n\n\n\n\nAccuracy\n\n\n\n\nGood results for most data sets\n\n\n\n\nFeasibility\n\n\n\n\nCustomized object and evaluation\n\n\nTurnable parameters\n\n\n\n\n\n\n\nXgboost Optimization \n2\n\n\n\n\nYou can use \nxgb.plot_important\n to decide how many features in your model.\n\n\nUse \nxgb.cv\n (\nexample\n) instead of \nxgb.train\n with \nwatchlist\n (\nexample\n)\n\n\n\n\nhttps://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12947/achieve-0-50776-on-the-leaderboard-in-a-minute-with-xgboost?page=5\n\n\nInstallation\n\n\nInstallation in Windows 64bit, Python 2.7, Anaconda\n\n\n\n\ngit clone https://github.com/dmlc/xgboost\n\n\ngit checkout 9bc3d16\n\n\nOpen project in \nxgboost/windows\n with Visual Studio 2013\n\n\nIn Visual Studio 2013, open \nConfiguration Manager...\n,\n\n\nchoose \nRelease\n in \nActive solution configuration\n\n\nchoose \nx64\n in \nActive solution platform\n\n\n\n\n\n\nRebuild \nxgboost\n, \nxgboost_wrapper\n\n\nCopy all file in \nxgboost/windows/x64/Release\n folder to \nxgboost/wrapper\n\n\nGo to \nxgboost/python-package\n, run command python \nsetup.py install\n\n\nCheck xgboost by running command \npython -c \"import xgboost\"\n\n\n\n\nExamples\n\n\nMulti class classification:\n\n\nUnderstanding XGBoost Model on Otto Dataset\n\n\nResources\n\n\n\n\nhttp://www.slideshare.net/ShangxuanZhang/xgboost\n\n\n\n\n\n\n\n\n\n\n\n\nyoutube, Kaggle Winning Solution Xgboost algorithm -- Let us learn from its author\n\n\n\n\n\n\nNotes on Parameter Tuning", 
            "title": "Ensemble"
        }, 
        {
            "location": "/ensemble/#ensemble", 
            "text": "", 
            "title": "Ensemble"
        }, 
        {
            "location": "/ensemble/#ensemble-algorithms-1", 
            "text": "Ensemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction.  Much effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.   Boosting  Bootstrapped Aggregation (Bagging)  AdaBoost  Stacked Generalization (blending)  Gradient Boosting Machines (GBM)  Gradient Boosted Regression Trees (GBRT)  Random Forest", 
            "title": "Ensemble Algorithms 1"
        }, 
        {
            "location": "/ensemble/#xgboost", 
            "text": "XGBoost is short for eXtreme gradient boosting.", 
            "title": "XGBoost"
        }, 
        {
            "location": "/ensemble/#features-1", 
            "text": "Easy to use  Easy to install  Highly developed R/python for users   Efficiency  Automatic parallel computation on a single machine  Can be run on a cluster.   Accuracy  Good results for most data sets   Feasibility  Customized object and evaluation  Turnable parameters", 
            "title": "Features 1"
        }, 
        {
            "location": "/ensemble/#xgboost-optimization-2", 
            "text": "You can use  xgb.plot_important  to decide how many features in your model.  Use  xgb.cv  ( example ) instead of  xgb.train  with  watchlist  ( example )   https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12947/achieve-0-50776-on-the-leaderboard-in-a-minute-with-xgboost?page=5", 
            "title": "Xgboost Optimization 2"
        }, 
        {
            "location": "/ensemble/#installation", 
            "text": "Installation in Windows 64bit, Python 2.7, Anaconda   git clone https://github.com/dmlc/xgboost  git checkout 9bc3d16  Open project in  xgboost/windows  with Visual Studio 2013  In Visual Studio 2013, open  Configuration Manager... ,  choose  Release  in  Active solution configuration  choose  x64  in  Active solution platform    Rebuild  xgboost ,  xgboost_wrapper  Copy all file in  xgboost/windows/x64/Release  folder to  xgboost/wrapper  Go to  xgboost/python-package , run command python  setup.py install  Check xgboost by running command  python -c \"import xgboost\"", 
            "title": "Installation"
        }, 
        {
            "location": "/ensemble/#examples", 
            "text": "Multi class classification:  Understanding XGBoost Model on Otto Dataset", 
            "title": "Examples"
        }, 
        {
            "location": "/ensemble/#resources", 
            "text": "http://www.slideshare.net/ShangxuanZhang/xgboost       youtube, Kaggle Winning Solution Xgboost algorithm -- Let us learn from its author    Notes on Parameter Tuning", 
            "title": "Resources"
        }, 
        {
            "location": "/evaluation/", 
            "text": "Evaluation\n\n\nBias - Variance", 
            "title": "Evaluation"
        }, 
        {
            "location": "/evaluation/#evaluation", 
            "text": "", 
            "title": "Evaluation"
        }, 
        {
            "location": "/evaluation/#bias-variance", 
            "text": "", 
            "title": "Bias - Variance"
        }, 
        {
            "location": "/app_reduction/", 
            "text": "Dimensionality Reduction\n\n\n\n\nDimensionality Reduction Algorithms\n\n\nLike clustering methods, dimensionality reduction seek and exploit the inherent structure in the data, but in this case in an unsupervised manner or order to summarise or describe data using less information.\n\n\nThis can be useful to visualize dimensional data or to simplify data which can then be used in a supervized learning method. Many of these methods can be adapted for use in classification and regression.\n\n\n\n\nPrincipal Component Analysis (PCA)\n\n\nPrincipal Component Regression (PCR)\n\n\nPartial Least Squares Regression (PLSR)\n\n\nSammon Mapping\n\n\nMultidimensional Scaling (MDS)\n\n\nProjection Pursuit\n\n\nLinear Discriminant Analysis (LDA)\n\n\nMixture Discriminant Analysis (MDA)\n\n\nQuadratic Discriminant Analysis (QDA)\n\n\nFlexible Discriminant Analysis (FDA)\n\n\n\n\nt-SNE\n\n\n\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE) \n1\n is a (prize-winning) technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. The technique can be implemented via Barnes-Hut approximations, allowing it to be applied on large real-world datasets. We applied it on data sets with up to 30 million examples. The technique and its variants are introduced in the following papers:\n\n\n\n\nL.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine Learning Research 15(Oct):3221-3245, 2014.  PDF [Supplemental material]\n\n\nL.J.P. van der Maaten and G.E. Hinton. Visualizing Non-Metric Similarities in Multiple Maps. Machine Learning 87(1):33-55, 2012.  PDF\n\n\nL.J.P. van der Maaten. Learning a Parametric Embedding by Preserving Local Structure. In Proceedings of the Twelfth International Conference on Artificial Intelligence \n Statistics (AI-STATS), JMLR W\nCP 5:384-391, 2009.  PDF\n\n\nL.J.P. van der Maaten and G.E. Hinton. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9(Nov):2579-2605, 2008.  PDF [Supplemental material] [Talk]\n\n\n\n\n\n\n\n\n\n\n\n\nt-SNE", 
            "title": "Dimensionality Reduction"
        }, 
        {
            "location": "/app_reduction/#dimensionality-reduction", 
            "text": "", 
            "title": "Dimensionality Reduction"
        }, 
        {
            "location": "/app_reduction/#dimensionality-reduction-algorithms", 
            "text": "Like clustering methods, dimensionality reduction seek and exploit the inherent structure in the data, but in this case in an unsupervised manner or order to summarise or describe data using less information.  This can be useful to visualize dimensional data or to simplify data which can then be used in a supervized learning method. Many of these methods can be adapted for use in classification and regression.   Principal Component Analysis (PCA)  Principal Component Regression (PCR)  Partial Least Squares Regression (PLSR)  Sammon Mapping  Multidimensional Scaling (MDS)  Projection Pursuit  Linear Discriminant Analysis (LDA)  Mixture Discriminant Analysis (MDA)  Quadratic Discriminant Analysis (QDA)  Flexible Discriminant Analysis (FDA)", 
            "title": "Dimensionality Reduction Algorithms"
        }, 
        {
            "location": "/app_reduction/#t-sne", 
            "text": "t-Distributed Stochastic Neighbor Embedding (t-SNE)  1  is a (prize-winning) technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. The technique can be implemented via Barnes-Hut approximations, allowing it to be applied on large real-world datasets. We applied it on data sets with up to 30 million examples. The technique and its variants are introduced in the following papers:   L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine Learning Research 15(Oct):3221-3245, 2014.  PDF [Supplemental material]  L.J.P. van der Maaten and G.E. Hinton. Visualizing Non-Metric Similarities in Multiple Maps. Machine Learning 87(1):33-55, 2012.  PDF  L.J.P. van der Maaten. Learning a Parametric Embedding by Preserving Local Structure. In Proceedings of the Twelfth International Conference on Artificial Intelligence   Statistics (AI-STATS), JMLR W CP 5:384-391, 2009.  PDF  L.J.P. van der Maaten and G.E. Hinton. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9(Nov):2579-2605, 2008.  PDF [Supplemental material] [Talk]       t-SNE", 
            "title": "t-SNE"
        }, 
        {
            "location": "/app_recommendation/", 
            "text": "Recommendation System\n\n\nIntroduction \n2\n\n\nTwo motivations for talking about recommender systems\n\n\n\n\nImportant application of ML systems\n\n\nMany technology companies find recommender systems to be absolutely key\n\n\nThink about websites (amazon, Ebay, iTunes genius)\n\n\nTry and recommend new content for you based on passed purchase\n\n\nSubstantial part of Amazon's revenue generation\n\n\n\n\n\n\n\n\n\n\nImprovement in recommender system performance can bring in more income\n\n\nKind of a funny problem\n\n\nIn academic learning, recommender systems receives a small amount of attention\n\n\nBut in industry it's an absolutely crucial tool\n\n\n\n\n\n\nTalk about the big ideas in machine learning\n\n\nNot so much a technique, but an idea\n\n\nAs soon, features are really important\n\n\nThere's a big idea in machine learning that for some problems you can learn what a good set of features are\n\n\nSo not select those features but learn them\n\n\n\n\n\n\nRecommender systems do this - try and identify the crucial and relevant features\n\n\n\n\n\n\n\n\nExample - predict movie ratings\n\n\n\n\nYou're a company who sells movies\n\n\nYou let users rate movies using a 1-5 star rating\n\n\nTo make the example nicer, allow 0-5 (makes math easier)\n\n\n\n\n\n\n\n\n\n\nYou have five movies\n\n\nAnd you have four users\n\n\n\n\nAdmittedly, business isn't going well, but you're optimistic about the future as a result of your truly outstanding (if limited) inventory\n\n\n\n\n\n\n\nTo introduce some notation\n\n\n\n\n$latex n_u$ - Number of users (called $?^{nu}$ occasionally as we can't subscript in superscript)\n\n\n$latex n_m$ - Number of movies\n\n\n$latex r(i, j)$ - 1 if user j has rated movie i (i.e. bitmap)\n\n\n$latex y(i,j)$ - rating given by user j to move i (defined only if $latex r(i,j) = 1$)\n\n\n\n\n\n\nSo for this example\n\n\n$latex n_u = 4$\n\n\n$latex n_m = 5$\n\n\nSummary of scoring\n\n\nAlice and Bob gave good ratings to rom coms, but low scores to action films\n\n\nCarol and Dave game good ratings for action films but low ratings for rom coms\n\n\n\n\n\n\nWe have the data given above\n\n\nThe problem is as follows\n\n\nGiven $latex r(i,j)$ and $latex y^{(i,j)}$ - go through and try and predict missing values (?s)\n\n\nCome up with a learning algorithm that can fill in these missing values\n\n\n\n\n\n\n\n\n\n\n\n\nKDD 2015 Tutorial:\u00a0\nShlomo Berkovsky and Jill Freyne, Web Personalisation and\u00a0Recommender Systems\n\n\n1. Approaches \n1\n\n\n\n\nAttribute-based Recommendations\n\n\n\n\nYou like action movies, starring Clint Eastwood, you might like \"Good, Bad and the Ugly\" (Netflix)\n\n\n\n\nItem Hierachy\n\n\n\n\nYou bought Printer you will also need ink (Bestbuy)\n\n\n\n\nAssociation Rules\n\n\nContent-Based Recommender Collaborative Filtering - Item-Item Similarity\n\n\n\n\nYou like Godfather so you will like Scarface (Netflix)\n\n\n\n\nCollaborative Filtering - User-User Similarity\n\n\n\n\nPeople like you who bought beer also bought diapers (Target)\n\n\n\n\nSocial+Interest Graph Based\n\n\n\n\nYour friends like Lady Gaga so you will like Lady Gaga (Facebook, Linkedin)\n\n\n\n\nModel Based\n\n\n\n\nTraining SVM, LDA, SVD for implicit features.\n\n\n\n\n2. Challenges\n\n\nKaggle Challenge:\n Million Song Dataset Challenge\n\n\n3. Articles\n\n\n\n\nHow Big Data is used in Recommendation Systems to change our lives\n\n\n\n\n4. Recommendation Interface\n\n\n4.1 Type of Input\n\n\n\n\npredictions\n\n\nrecommendations\n\n\nfiltering\n\n\norganic vs explicit presentation\n\n\n\n\n4.2 Type of Output\n\n\n\n\nexplicit\n\n\nimplicit\n\n\n\n\nApriori\n\n\nhttps://en.wikipedia.org/wiki/Apriori_algorithm\n\n\nhttps://github.com/asaini/Apriori\n\n\nItem item collaborative filtering\n\n\nWorks when |U| \n |I|\n\n\n\n\nitems dont change much\n\n\n\n\nRS: Examples\n\n\nGoogle News\n\n\n\n\nRS: Association Rules\n\n\n\n\nContent Based Recommendation\n\n\nUser\u2013User Collaborative Filtering\n\n\nUser - User \n1\n\n\nUser user look simular in row space\n\n\n$p_{u, i} = \\overline{r_u} + \\frac{\\sum_{u' \\in N} s(u, u') (r_{u', i} - \\overline{r_u'})}{\\sum_{u' \\in N}|s(u, u')|}\ns=2$\n\n\n\n\n\n\n\n\n\n\nhttp://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf\n\n\n\n\n\n\nmlclass lecture notes, Recommender Systems", 
            "title": "Recommendation System"
        }, 
        {
            "location": "/app_recommendation/#recommendation-system", 
            "text": "", 
            "title": "Recommendation System"
        }, 
        {
            "location": "/app_recommendation/#introduction-2", 
            "text": "Two motivations for talking about recommender systems   Important application of ML systems  Many technology companies find recommender systems to be absolutely key  Think about websites (amazon, Ebay, iTunes genius)  Try and recommend new content for you based on passed purchase  Substantial part of Amazon's revenue generation      Improvement in recommender system performance can bring in more income  Kind of a funny problem  In academic learning, recommender systems receives a small amount of attention  But in industry it's an absolutely crucial tool    Talk about the big ideas in machine learning  Not so much a technique, but an idea  As soon, features are really important  There's a big idea in machine learning that for some problems you can learn what a good set of features are  So not select those features but learn them    Recommender systems do this - try and identify the crucial and relevant features", 
            "title": "Introduction 2"
        }, 
        {
            "location": "/app_recommendation/#example-predict-movie-ratings", 
            "text": "You're a company who sells movies  You let users rate movies using a 1-5 star rating  To make the example nicer, allow 0-5 (makes math easier)      You have five movies  And you have four users   Admittedly, business isn't going well, but you're optimistic about the future as a result of your truly outstanding (if limited) inventory    To introduce some notation   $latex n_u$ - Number of users (called $?^{nu}$ occasionally as we can't subscript in superscript)  $latex n_m$ - Number of movies  $latex r(i, j)$ - 1 if user j has rated movie i (i.e. bitmap)  $latex y(i,j)$ - rating given by user j to move i (defined only if $latex r(i,j) = 1$)    So for this example  $latex n_u = 4$  $latex n_m = 5$  Summary of scoring  Alice and Bob gave good ratings to rom coms, but low scores to action films  Carol and Dave game good ratings for action films but low ratings for rom coms    We have the data given above  The problem is as follows  Given $latex r(i,j)$ and $latex y^{(i,j)}$ - go through and try and predict missing values (?s)  Come up with a learning algorithm that can fill in these missing values       KDD 2015 Tutorial:\u00a0 Shlomo Berkovsky and Jill Freyne, Web Personalisation and\u00a0Recommender Systems", 
            "title": "Example - predict movie ratings"
        }, 
        {
            "location": "/app_recommendation/#1-approaches-1", 
            "text": "Attribute-based Recommendations   You like action movies, starring Clint Eastwood, you might like \"Good, Bad and the Ugly\" (Netflix)   Item Hierachy   You bought Printer you will also need ink (Bestbuy)   Association Rules  Content-Based Recommender Collaborative Filtering - Item-Item Similarity   You like Godfather so you will like Scarface (Netflix)   Collaborative Filtering - User-User Similarity   People like you who bought beer also bought diapers (Target)   Social+Interest Graph Based   Your friends like Lady Gaga so you will like Lady Gaga (Facebook, Linkedin)   Model Based   Training SVM, LDA, SVD for implicit features.", 
            "title": "1. Approaches 1"
        }, 
        {
            "location": "/app_recommendation/#2-challenges", 
            "text": "Kaggle Challenge:  Million Song Dataset Challenge", 
            "title": "2. Challenges"
        }, 
        {
            "location": "/app_recommendation/#3-articles", 
            "text": "How Big Data is used in Recommendation Systems to change our lives", 
            "title": "3. Articles"
        }, 
        {
            "location": "/app_recommendation/#4-recommendation-interface", 
            "text": "", 
            "title": "4. Recommendation Interface"
        }, 
        {
            "location": "/app_recommendation/#41-type-of-input", 
            "text": "predictions  recommendations  filtering  organic vs explicit presentation", 
            "title": "4.1 Type of Input"
        }, 
        {
            "location": "/app_recommendation/#42-type-of-output", 
            "text": "explicit  implicit", 
            "title": "4.2 Type of Output"
        }, 
        {
            "location": "/app_recommendation/#apriori", 
            "text": "https://en.wikipedia.org/wiki/Apriori_algorithm  https://github.com/asaini/Apriori", 
            "title": "Apriori"
        }, 
        {
            "location": "/app_recommendation/#item-item-collaborative-filtering", 
            "text": "Works when |U|   |I|   items dont change much", 
            "title": "Item item collaborative filtering"
        }, 
        {
            "location": "/app_recommendation/#rs-examples", 
            "text": "Google News", 
            "title": "RS: Examples"
        }, 
        {
            "location": "/app_recommendation/#rs-association-rules", 
            "text": "", 
            "title": "RS: Association Rules"
        }, 
        {
            "location": "/app_recommendation/#content-based-recommendation", 
            "text": "", 
            "title": "Content Based Recommendation"
        }, 
        {
            "location": "/app_recommendation/#useruser-collaborative-filtering", 
            "text": "", 
            "title": "User\u2013User Collaborative Filtering"
        }, 
        {
            "location": "/app_recommendation/#user-user-1", 
            "text": "User user look simular in row space  $p_{u, i} = \\overline{r_u} + \\frac{\\sum_{u' \\in N} s(u, u') (r_{u', i} - \\overline{r_u'})}{\\sum_{u' \\in N}|s(u, u')|} s=2$      http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf    mlclass lecture notes, Recommender Systems", 
            "title": "User - User 1"
        }, 
        {
            "location": "/app_anomaly/", 
            "text": "Anomaly\n\n\n\n\n\n\nMotivation and Examples\n\n\nAlgorithms\n\n\nEvaluation\n\n\n\n\nAD: Examples\n\n\nProblem motivation \n1\n\n\n\n\nAnomaly detection is a reasonably commonly used type of machine learning application\n\n\nCan be thought of as a solution to an unsupervised learning problem\n\n\nBut, has aspects of supervised learning\n\n\n\n\n\n\nWhat is anomaly detection?\n\n\nImagine you're an aircraft engine manufacturer\n\n\nAs engines roll off your assembly line you're doing QA\n\n\nMeasure some features from engines (e.g. heat generated and vibration)\n\n\n\n\n\n\nYou now have a dataset of x1 to xm (i.e. m engines were tested)\n\n\nSay we plot that dataset\n\n\n\n\n\n\n\nNext day you have a new engine\n\n\nAn anomaly detection method is used to see if the new engine is anomalous (when compared to the previous engines)\n\n\n\n\n\n\nIf the new engine looks like this;\n\n\n\nProbably OK - looks like the ones we've seen before\n\n\n\n\n\n\nBut if the engine looks like this\n\n\n\nUh oh! - this looks like an anomalous data-point\n\n\n\n\n\n\nMore formally\n\n\nWe have a dataset which contains normal (data)\n\n\nHow we ensure they're normal is up to us\n\n\nIn reality it's OK if there are a few which aren't actually normal\n\n\n\n\n\n\nUsing that dataset as a reference point we can see if other examples are anomalous\n\n\n\n\n\n\nHow do we do this?\n\n\nFirst, using our training dataset we build a model\n\n\nWe can access this model using p(x)\n\n\nThis asks, \"What is the probability that example x is normal\"\n\n\n\n\n\n\nHaving built a model\n\n\nif $latex p(x_{test}) \n \\epsilon$ --\n flag this as an anomaly\n\n\nif $latex p(x_{test}) \\ge \\epsilon$ --\n this is OK\n\n\n\u03b5 is some threshold probability value which we define, depending on how sure we need/want to be\n\n\n\n\n\n\nWe expect our model to (graphically) look something like this;\n\n\n\ni.e. this would be our model if we had 2D data\n\n\n\n\n\n\n\n\n\n\n\n\nExamples \n1\n\n\n\n\nFraud detection\n\n\nUsers have activity associated with them, such as\n\n\nLength on time on-line\n\n\nLocation of login\n\n\nSpending frequency\n\n\n\n\n\n\nUsing this data we can build a model of what normal users' activity is like\n\n\nWhat is the probability of \"normal\" behavior?\n\n\nIdentify unusual users by sending their data through the model\n\n\nFlag up anything that looks a bit weird\n\n\nAutomatically block cards/transactions\n\n\n\n\n\n\n\n\n\n\nManufacturing\n\n\nAlready spoke about aircraft engine example\n\n\n\n\n\n\nMonitoring computers in data center\n\n\nIf you have many machines in a cluster\n\n\nComputer features of machine\n\n\n$latex x_1$ = memory use\n\n\n$latex x_2$ = number of disk accesses/sec\n\n\n$latex x_3$ = CPU load\n\n\n\n\n\n\nIn addition to the measurable features you can also define your own complex features\n\n\n$latex x_4$ = CPU load/network traffic\n\n\n\n\n\n\nIf you see an anomalous machine\n\n\nMaybe about to fail\n\n\nLook at replacing bits from it\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlclass lecture notes, Anomaly Detection", 
            "title": "Anomaly Detection"
        }, 
        {
            "location": "/app_anomaly/#anomaly", 
            "text": "Motivation and Examples  Algorithms  Evaluation", 
            "title": "Anomaly"
        }, 
        {
            "location": "/app_anomaly/#ad-examples", 
            "text": "", 
            "title": "AD: Examples"
        }, 
        {
            "location": "/app_anomaly/#problem-motivation-1", 
            "text": "Anomaly detection is a reasonably commonly used type of machine learning application  Can be thought of as a solution to an unsupervised learning problem  But, has aspects of supervised learning    What is anomaly detection?  Imagine you're an aircraft engine manufacturer  As engines roll off your assembly line you're doing QA  Measure some features from engines (e.g. heat generated and vibration)    You now have a dataset of x1 to xm (i.e. m engines were tested)  Say we plot that dataset    Next day you have a new engine  An anomaly detection method is used to see if the new engine is anomalous (when compared to the previous engines)    If the new engine looks like this;  Probably OK - looks like the ones we've seen before    But if the engine looks like this  Uh oh! - this looks like an anomalous data-point    More formally  We have a dataset which contains normal (data)  How we ensure they're normal is up to us  In reality it's OK if there are a few which aren't actually normal    Using that dataset as a reference point we can see if other examples are anomalous    How do we do this?  First, using our training dataset we build a model  We can access this model using p(x)  This asks, \"What is the probability that example x is normal\"    Having built a model  if $latex p(x_{test})   \\epsilon$ --  flag this as an anomaly  if $latex p(x_{test}) \\ge \\epsilon$ --  this is OK  \u03b5 is some threshold probability value which we define, depending on how sure we need/want to be    We expect our model to (graphically) look something like this;  i.e. this would be our model if we had 2D data", 
            "title": "Problem motivation 1"
        }, 
        {
            "location": "/app_anomaly/#examples-1", 
            "text": "Fraud detection  Users have activity associated with them, such as  Length on time on-line  Location of login  Spending frequency    Using this data we can build a model of what normal users' activity is like  What is the probability of \"normal\" behavior?  Identify unusual users by sending their data through the model  Flag up anything that looks a bit weird  Automatically block cards/transactions      Manufacturing  Already spoke about aircraft engine example    Monitoring computers in data center  If you have many machines in a cluster  Computer features of machine  $latex x_1$ = memory use  $latex x_2$ = number of disk accesses/sec  $latex x_3$ = CPU load    In addition to the measurable features you can also define your own complex features  $latex x_4$ = CPU load/network traffic    If you see an anomalous machine  Maybe about to fail  Look at replacing bits from it           mlclass lecture notes, Anomaly Detection", 
            "title": "Examples 1"
        }, 
        {
            "location": "/app_nlp/", 
            "text": "Natural Language Processing (NLP)\n\n\n\n\nNLP Tasks\n\n\nMorphological Analysis\n\n\nDiscourse Analysis\n\n\nSentiment Analysis\n\n\n\nMetaMind\n,\u00a0@RichardSocher\n\n\nNamed Entity Recognition\n\n\n\nKDD 2015 Tutorial: Automatic Entity Recognition and Typing from Massive Text Corpora - A Phrase and Network Mining Approach\n\n\nRelationship Extraction\n\n\n\nAlchemyAPI\n\n\nNLP Applications\n\n\nInformation Retrieval (IR)\n\n\nInformation retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on metadata or on full-text (or other content-based) indexing.\n\n\nInformation Extraction (IE)\n\n\nInformation extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP).\n\n\nMachine Translation\n\n\nQuestion Answering (QA)\n\n\nQuestion answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.\n\n\n\n\nIs paris capital of France?\nYES\n\n\n\n\nDeep Learning\n\n\nA Primer on Neural Network Models for Natural Language Processing\n\n\nCourses\n\n\n\n\nCS224n: Natural Language Processing (\nwebsite\n, \nvideo\n)\n\n\nCS224d: Deep Learning for Natural Language Processing\n\n\n\n\nPOS Tagging\n\n\n\n\nA Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text in some language and assigns parts of speech to each word (and other token), such as noun, verb, adjective, etc., although generally computational applications use more fine-grained POS tags like 'noun-plural'.\n\n\nMethods\n\n\n\n\nSequence Classification \n1\n\n\n\n\nTextual entailment\n\n\n\n\nTextual entailment (TE) in natural language processing is a directional relation between text fragments. The relation holds whenever the truth of one text fragment follows from another text. In the TE framework, the entailing and entailed texts are termed text (t) and hypothesis (h), respectively. Textual entailment is not the same as pure logical entailment- it has a more relaxed definition: \"t entails h\" (t \u21d2 h) if, typically, a human reading t would infer that h is most likely true. The relation is directional because even if \"t entails h\", the reverse \"h entails t\" is much less certain. \n1\n\n\nLanguage Modeling\n\n\nLanguage Models \n1\n\n\n\n\nN-Gram\n\n\nTfidf\n\n\nword2vec\n\n\n\n\nN-Gram Model\n\n\nTfidf\n\n\ntf\u2013idf, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. \n1\n\n\ntf\n (Term Frequence)\n\n\nidf\n (Inverse Document Frequency)\n\n\nHow term is important in corpus?\n\n\ntf-idf\n - Term frequency\u2013Inverse document frequency\n\n\nHow term is important in document?\n\n\nPython Lab\n\n\nWe exam a corpus with 2 documents\n\n\n\n\nDoc 1: \nthis is a sample\n\n\nDoc 2: \nthis is another sample\n\n\n\n\nTf\n\n\n\n\n\n\nVocab\n\n\n$latex tf=f_{t,d}$\n\n\n\n\n\n\nthis\n\n\n\n\n\n\n\n\nis\n\n\n\n\n\n\n\n\na\n\n\n\n\n\n\n\n\nsample\n\n\n\n\n\n\n\n\nanother\n\n\n\n\n\n\n\n\n\n\n\n[code lang=\"python\"]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nX = [\"this is a sample\", \"this is another example\"]\nvectorizer = TfidfVectorizer()\nvectorizer.fit_transform(X)\n[/code]\n\n\nNow we find what are important terms of this corpus:\n\n\n[code lang=\"python\"]\nfor term in vectorizer.vocabulary_.keys():\n    index = vectorizer.vocabulary_[term]\n    score = vectorizer.\ntfidf.idf\n[index]\n    print \"%10s: %2.2f\" % (term, score)\n[/code]\n[code]\n      this: 1.00\n    sample: 1.41\n        is: 1.00\n   example: 1.41\n   another: 1.41\n[/code]\n\n\nsample\n,\nexample\n and \nanother\n are important term in this corpus.\n\n\nNext, we look at 2 more documents, and find what are import terms in those documents\n\n\n[code lang=\"python\"]\nprint vectorizer.vocabulary_\nprint vectorizer.transform([\"another sample\", \"this example\"])\n[/code]\n\n\n[code]\n{u'this': 4, u'sample': 3, u'is': 2, u'example': 1, u'another': 0}\n  (0, 3)    0.707106781187\n  (0, 0)    0.707106781187\n  (1, 4)    0.579738671538\n  (1, 1)    0.814802474667\n[/code]\n\n\nIn document 1, \nsample\n (index 3) and \nanother\n (index 0) are equally, but in document 2, \nexample\n (index 0) is more important than \nthis\n (index 4). The reasons is \nthis\n appears in whole corpus, there for it doesn't tell us any more information.\n\n\nUnigram Bigram Model\n\n\nNear-Duplicates\n\n\nSimhash\n\n\nDetecting Near-Duplicates for Web Crawling\n\n\nSimHash\n\n\nWord2vec\n\n\nInstallation\n\n\n[code]\nconda install gensim\n[/code]\n\n\nLab\n\n\nStep 1: I download some articles about Ha\u0300 N\u00f4\u0323i (the captial of Vi\u00ea\u0323t Nam)\n\n\nStep 2: I use vnTokenizer to tokenize words\n\n\nStep 3: I train Word2Vec model\n\n\nResources\n\n\nWord2vec-pride-vis \n[code]\n/code\n, \ninteractive visualization\n\n\nDocument Classification\n\n\nDocument classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done \"manually\" (or \"intellectually\") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification. \n2\n\n\nProcess \n1\n\n\n\n\nStep 1: Generate document features: TFidf Model,\n\n\nStep 2: Fit features to a classifier: Multinomial Naive Bayes, Maxent Classifier, DecisionTreeClassifier\n\n\nStep 3: Evaluating: use F1 score\n\n\n\n\nDocument Clustering\n\n\nDocument clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in automatic document organization, topic extraction and fast information retrieval or filtering. \n1\n\n\n\n\nTopicModel \n LDA\n\n\n\n\nTask: Related terms in documents\n\n\nAlgorithm to find related words in a text\n\n\nHow to find related terms in documents\n\n\nTopic Models: LDA\n\n\nSentiment Analysis\n\n\nName Entity Recognization\n\n\n\n\nNamed-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify elements in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. \n1\n\n\nTutorial\n\n\n9 - 3 - Sequence Models for Named Entity Recognition-NLP-Professor Dan Jurafsky \n Chris Manning\n\n\nRelation Extraction\n\n\nSentence Segmentation\n\n\n\n\nSentence segmentation is the problem of dividing a string of written language into its component sentences. In English and some other languages, using punctuation, particularly the full stop/period character is a reasonable approximation. However even in English this problem is not trivial due to the use of the full stop character for abbreviations, which may or may not also terminate a sentence.\n\n\nFor example Mr. is not its own sentence in \"Mr. Smith went to the shops in Jones Street.\" When processing plain text, tables of abbreviations that contain periods can help prevent incorrect assignment of sentence boundaries.\n\n\nAs with word segmentation, not all written languages contain punctuation characters which are useful for approximating sentence boundaries.\n\n\nEnglish NLP\n\n\nDictionary / Wordnet \n1\n\n\nCorpus\n\n\n\n\nEnglish Wikipedia\n\n\n\n\nTools\n\n\n\n\ngensims\n\n\nwiki\n\n\n\n\nVietnamese NLP\n\n\nH\u1ed3 T\u00fa B\u1ea3o, V\u1ec1 x\u1eed l\u00fd ti\u1ebfng Vi\u1ec7t trong c\u00f4ng ngh\u1ec7 th\u00f4ng tin\n\n\nPart I. Core Problems\n\n\n1.1 Dictionaries\n\n\n\n\n2004, H\u1ed3 Ng\u1ecdc \u0110\u1ee9c, The Free Vietnamese Dictionary Project\n\n\n\n\n1.2 Wordnet\n\n\n\n\nviet wordnet\n\n\n\n\n1.3 Corpus\n\n\nVNESEcorpus\n, 650.000 sentences, 10.000 articles from vietnamnet.vn, dantri.com.vn, nhanhdan.com.vn. Size: 64.59 Mb\n\n\nVNTQcorpus(small)\n, 300.000 sentences, 1.000 articles from vnthuquan.net\nSize: ~35 Mb\n\n\nVNTQcorpus(big)\n, 1.750.000 sentences, 13.000 articles from vnthuquan.net, Size: ~240 Mb\n\n\n1.4 \nSentence Segmentation\n\n\nUnknown\n\n\n1.5 Word Segmentation\n\n\n \nVitk\n \nspark\n\n\nAuthors: \nLe Hong Phuong\n\nDate: May 08, 2016\n\n\nThis is the first release of a Vietnamese text processing toolkit, which is called \"Vitk\", developed by Phuong LE-HONG at College of Natural Sciences, Vietnam National University, Hanoi.\n\n\n \nvnTokenizer\n \njava\n\n\nAuthors: \nLe Hong Phuong\n\nDate: September 28, 2009\n\n\nvnTokenizer is a software for tokenizing Vietnamese texts. It segments Vietnamese texts into lexical units (words, names, dates, numbers and other regular expressions) with a high accuracy, of about 98% on a test set extracted from the Vietnamese treebank.\n\n\n  \nJVnSegmenter\n \njava\n\n\nAuthors: Cam-Tu Nguyen (ncamtu@gmail.com), Xuan-Hieu Phan (pxhieu@gmail.com)\nDate: Mar 24, 2007\n\n\nA Java-based Vietnamese Word Segmentation Tool\n\n\n \nDongDu\n \nC++\n\n\nAuthors: rockkhuya(\nrockkhuya@gmail.com\n)\n\n\nA Vietnamese word segmentation tool.\n\n\n \nRoy_VnTokenizer\n \npython\n\n\nAuthors: Anindya Roy\nDate: Jan 22, 2014\n\n\nVietnamese tokenization\n\n\n \nOnline Tool from VLSP\n \nonline\n\n\nNot Available\n\n\n1.6 Part-of-speech tagging (POS Tagging)\n\n\nVCCorp 2016: 94.5% \n1\n\nVitk 2016: accurary 95% (Vietnamese Tree Bank)\n\n\n \nVitk\n \nspark\n\n\nAuthors: \nLe Hong Phuong\n\nDate: May 08, 2016\n\n\nThe part-of-speech tagger of Vitk can tag about 1,105,000 tokens per second, on a single machine, giving an accuracy of about 95% on the Vietnamese treebank.\n\n\n \nvnTagger\n \njava\n\n\nVietnamese part-of-speech tagging\n\n\nAuthors: \nLe Hong Phuong\n\nDate: Aug 05, 2010\n\n\nPaper\n\n\n\n\n \nLe Hong Phuong, 2010, An empirical study of maximum entropy approach for part-of-speech tagging of Vietnamese texts\n\n\n\n\n1.7 Coreference\n\n\nVCCorp 2016: 57% \n1\n\n\nThesis \n Papers\n:\n\n\n\n\n \n2011, Gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n \u0111\u1ed3ng tham chi\u1ebfu trong v\u0103n b\u1ea3n ti\u1ebfng vi\u1ec7t d\u1ef1a v\u00e0o ph\u01b0\u01a1ng ph\u00e1p m\u00e1y vector h\u1ed7 tr\u1ee3 SVM\n\n\n\n\n1.8 Dependency Grammar\n\n\nVCCorp 2016: 73% \n1\n\n\n \n2013, Nguy\u1ec5n Vi D\u01b0\u01a1ng, Nguy\u1ec5n Th\u1ecb \u0110\u1ea3m, B\u1ed9 chuy\u1ec3n \u0111\u1ed5i t\u1eeb v\u0103n ph\u1ea1m th\u00e0nh ph\u1ea7n sang v\u0103n ph\u1ea1m ph\u1ee5 thu\u1ed9c cho ti\u1ebfng Vi\u1ec7t\n\n\n1.9 Chunking\n\n\nVCCorp 2016: 83% \n1\n\n\n1.10 Named Entity Recognition (NER)\n\n\nVCCorp 2016: 84.8% \n1\n\n\nPapers\n:\n\n\n\n\n \n2007, Named Entity Recognition in Vietnamese documents\n\n\n \n2005, Named Entity Recognition in Vietnamese Free-Text and Web Documents Using Conditional Random Fields\n\n\n\n\n1.11 Relations Extraction Systems\n\n\nUnknown\n\n\n1.12 Sentiment Analysis\n\n\n \nSentiment Analysis\n \njava\n\n\nAuthors: \nEpi Lab\n\nDate: Aug 01, 2013\n\n\n1.13 Language Identification\n\n\nUnknown\n\n\nPart 2. Vietnamse NLP Groups\n\n\nGroups\n\n\n\n\nvnlp.net, (2010-now)\n\n\nkde lab, (2014-now)\n\n\n\n\nPeople\n\n\n\n\nAssoc. Prof. Ha Quang Thuy\n\n\nProf. Tu-Bao Ho\n\n\nKhoat Than\n\n\nCam Tu Nguyen\n\n\nCAM-TU NGUYEN\n\n\nLe Hong Phuong\n\n\nPhan Xuan Hieu\n\n\nTr\u1ea7n Mai V\u0169\n\n\nNguy\u1ec5n Ki\u00eam Hi\u1ebfu\n\n\n\n\nPart 3. Applications\n\n\nVAV - Tr\u1ee3 l\u00fd \u1ea3o cho ng\u01b0\u1eddi Vi\u1ec7t\n\n\nDate: Nov 2015 - now\n\n\n\n\nMDN-Team, Khoa CNTT, Tr\u01b0\u1eddng \u0110H C\u00f4ng ngh\u1ec7, \u0110HQG HN Tools\n\n\n\n\nB\u1ea1n \u0111ang ngh\u0129 \u0111\u1ebfn m\u1ed9t \u1ee9ng d\u1ee5ng th\u00f4ng minh tr\u00ean di \u0111\u1ed9ng cho ph\u00e9p b\u1ea1n t\u01b0\u01a1ng t\u00e1c b\u1eb1ng gi\u1ecdng n\u00f3i \u0111\u1ec3 h\u1eb9n chu\u00f4ng b\u00e1o th\u1ee9c, \u0111\u1eb7t l\u1ecbch cho m\u1ed9t cu\u1ed9c h\u1ecdp, b\u1eadt \u0111\u1ecbnh v\u1ecb, g\u1ecdi \u0111i\u1ec7n cho ai \u0111\u00f3, truy c\u1eadp m\u1ed9t trang web b\u1ea5t k\u1ef3, t\u00ecm \u0111\u01b0\u1eddng tr\u00ean b\u1ea3n \u0111\u1ed3, \u0111\u1ecbnh v\u1ecb c\u00e2y ATM c\u1ee7a m\u1ed9t ng\u00e2n h\u00e0ng n\u00e0o \u0111\u00f3 g\u1ea7n v\u1edbi b\u1ea1n, hay th\u01b0\u1edfng th\u1ee9c m\u1ed9t b\u1ea3n nh\u1ea1c m\u00ecnh y\u00eau th\u00edch \u2026 \u1ee8ng d\u1ee5ng Tr\u1ee3 l\u00fd \u1ea3o VAV ch\u00ednh l\u00e0 c\u00e2u tr\u1ea3 l\u1eddi cho b\u1ea1n.\n\u0110\u01b0\u1ee3c thi\u1ebft k\u1ebf v\u00e0 ph\u00e1t tri\u1ec3n d\u1ef1a tr\u00ean c\u00e1c k\u1ef9 thu\u1eadt tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o (h\u1ecdc m\u00e1y, ph\u00e2n t\u00edch v\u00e0 hi\u1ec3u ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean), VAV c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c \u00fd \u0111\u1ecbnh c\u1ee7a b\u1ea1n d\u00f9 b\u1ea1n di\u1ec5n \u0111\u1ea1t c\u00e2u l\u1ec7nh c\u1ee7a m\u00ecnh theo nhi\u1ec1u c\u00e1ch kh\u00e1c nhau m\u00e0 kh\u00f4ng c\u1ea7n tu\u00e2n theo b\u1ea5t k\u1ef3 khu\u00f4n m\u1eabu n\u00e0o cho tr\u01b0\u1edbc. Nh\u1eefng g\u00ec VAV h\u01b0\u1edbng t\u1edbi l\u00e0 tr\u1edf th\u00e0nh m\u1ed9t tr\u1ee3 l\u00fd \u1ea3o th\u00f4ng minh gi\u00fap b\u1ea1n th\u1ef1c hi\u1ec7n nh\u1eefng \u0111i\u1ec1u m\u00ecnh mu\u1ed1n v\u00e0 l\u00e0 m\u1ed9t ng\u01b0\u1eddi \u0111\u1ed3ng h\u00e0nh th\u00e2n thi\u1ec7n, d\u00ed d\u1ecfm b\u00ean b\u1ea1n.\n\n\n\n\n\n\n\n\n\n\n\n\n2016, Big Challenges for Data Scientists at VCCORP\n\n\n\n\n\n\nDocument classification", 
            "title": "Natural Language Processing"
        }, 
        {
            "location": "/app_nlp/#natural-language-processing-nlp", 
            "text": "", 
            "title": "Natural Language Processing (NLP)"
        }, 
        {
            "location": "/app_nlp/#nlp-tasks", 
            "text": "Morphological Analysis  Discourse Analysis", 
            "title": "NLP Tasks"
        }, 
        {
            "location": "/app_nlp/#nlp-applications", 
            "text": "Information Retrieval (IR)  Information retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on metadata or on full-text (or other content-based) indexing.  Information Extraction (IE)  Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP).  Machine Translation  Question Answering (QA)  Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.   Is paris capital of France?\nYES", 
            "title": "NLP Applications"
        }, 
        {
            "location": "/app_nlp/#deep-learning", 
            "text": "A Primer on Neural Network Models for Natural Language Processing", 
            "title": "Deep Learning"
        }, 
        {
            "location": "/app_nlp/#courses", 
            "text": "CS224n: Natural Language Processing ( website ,  video )  CS224d: Deep Learning for Natural Language Processing", 
            "title": "Courses"
        }, 
        {
            "location": "/app_nlp/#pos-tagging", 
            "text": "A Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text in some language and assigns parts of speech to each word (and other token), such as noun, verb, adjective, etc., although generally computational applications use more fine-grained POS tags like 'noun-plural'.", 
            "title": "POS Tagging"
        }, 
        {
            "location": "/app_nlp/#methods", 
            "text": "Sequence Classification  1", 
            "title": "Methods"
        }, 
        {
            "location": "/app_nlp/#textual-entailment", 
            "text": "Textual entailment (TE) in natural language processing is a directional relation between text fragments. The relation holds whenever the truth of one text fragment follows from another text. In the TE framework, the entailing and entailed texts are termed text (t) and hypothesis (h), respectively. Textual entailment is not the same as pure logical entailment- it has a more relaxed definition: \"t entails h\" (t \u21d2 h) if, typically, a human reading t would infer that h is most likely true. The relation is directional because even if \"t entails h\", the reverse \"h entails t\" is much less certain.  1", 
            "title": "Textual entailment"
        }, 
        {
            "location": "/app_nlp/#language-modeling", 
            "text": "Language Models  1   N-Gram  Tfidf  word2vec", 
            "title": "Language Modeling"
        }, 
        {
            "location": "/app_nlp/#n-gram-model", 
            "text": "", 
            "title": "N-Gram Model"
        }, 
        {
            "location": "/app_nlp/#tfidf", 
            "text": "tf\u2013idf, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.  1  tf  (Term Frequence)  idf  (Inverse Document Frequency)  How term is important in corpus?  tf-idf  - Term frequency\u2013Inverse document frequency  How term is important in document?", 
            "title": "Tfidf"
        }, 
        {
            "location": "/app_nlp/#python-lab", 
            "text": "We exam a corpus with 2 documents   Doc 1:  this is a sample  Doc 2:  this is another sample", 
            "title": "Python Lab"
        }, 
        {
            "location": "/app_nlp/#tf", 
            "text": "Vocab  $latex tf=f_{t,d}$    this     is     a     sample     another      [code lang=\"python\"]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nX = [\"this is a sample\", \"this is another example\"]\nvectorizer = TfidfVectorizer()\nvectorizer.fit_transform(X)\n[/code]  Now we find what are important terms of this corpus:  [code lang=\"python\"]\nfor term in vectorizer.vocabulary_.keys():\n    index = vectorizer.vocabulary_[term]\n    score = vectorizer. tfidf.idf [index]\n    print \"%10s: %2.2f\" % (term, score)\n[/code]\n[code]\n      this: 1.00\n    sample: 1.41\n        is: 1.00\n   example: 1.41\n   another: 1.41\n[/code]  sample , example  and  another  are important term in this corpus.  Next, we look at 2 more documents, and find what are import terms in those documents  [code lang=\"python\"]\nprint vectorizer.vocabulary_\nprint vectorizer.transform([\"another sample\", \"this example\"])\n[/code]  [code]\n{u'this': 4, u'sample': 3, u'is': 2, u'example': 1, u'another': 0}\n  (0, 3)    0.707106781187\n  (0, 0)    0.707106781187\n  (1, 4)    0.579738671538\n  (1, 1)    0.814802474667\n[/code]  In document 1,  sample  (index 3) and  another  (index 0) are equally, but in document 2,  example  (index 0) is more important than  this  (index 4). The reasons is  this  appears in whole corpus, there for it doesn't tell us any more information.", 
            "title": "Tf"
        }, 
        {
            "location": "/app_nlp/#unigram-bigram-model", 
            "text": "", 
            "title": "Unigram Bigram Model"
        }, 
        {
            "location": "/app_nlp/#near-duplicates", 
            "text": "Simhash  Detecting Near-Duplicates for Web Crawling  SimHash", 
            "title": "Near-Duplicates"
        }, 
        {
            "location": "/app_nlp/#word2vec", 
            "text": "", 
            "title": "Word2vec"
        }, 
        {
            "location": "/app_nlp/#installation", 
            "text": "[code]\nconda install gensim\n[/code]", 
            "title": "Installation"
        }, 
        {
            "location": "/app_nlp/#lab", 
            "text": "Step 1: I download some articles about Ha\u0300 N\u00f4\u0323i (the captial of Vi\u00ea\u0323t Nam)  Step 2: I use vnTokenizer to tokenize words  Step 3: I train Word2Vec model", 
            "title": "Lab"
        }, 
        {
            "location": "/app_nlp/#resources", 
            "text": "Word2vec-pride-vis  [code] /code ,  interactive visualization", 
            "title": "Resources"
        }, 
        {
            "location": "/app_nlp/#document-classification", 
            "text": "Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done \"manually\" (or \"intellectually\") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.  2", 
            "title": "Document Classification"
        }, 
        {
            "location": "/app_nlp/#process-1", 
            "text": "Step 1: Generate document features: TFidf Model,  Step 2: Fit features to a classifier: Multinomial Naive Bayes, Maxent Classifier, DecisionTreeClassifier  Step 3: Evaluating: use F1 score", 
            "title": "Process 1"
        }, 
        {
            "location": "/app_nlp/#document-clustering", 
            "text": "Document clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in automatic document organization, topic extraction and fast information retrieval or filtering.  1   TopicModel   LDA", 
            "title": "Document Clustering"
        }, 
        {
            "location": "/app_nlp/#task-related-terms-in-documents", 
            "text": "Algorithm to find related words in a text  How to find related terms in documents", 
            "title": "Task: Related terms in documents"
        }, 
        {
            "location": "/app_nlp/#topic-models-lda", 
            "text": "", 
            "title": "Topic Models: LDA"
        }, 
        {
            "location": "/app_nlp/#sentiment-analysis", 
            "text": "", 
            "title": "Sentiment Analysis"
        }, 
        {
            "location": "/app_nlp/#name-entity-recognization", 
            "text": "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify elements in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.  1", 
            "title": "Name Entity Recognization"
        }, 
        {
            "location": "/app_nlp/#tutorial", 
            "text": "9 - 3 - Sequence Models for Named Entity Recognition-NLP-Professor Dan Jurafsky   Chris Manning", 
            "title": "Tutorial"
        }, 
        {
            "location": "/app_nlp/#relation-extraction", 
            "text": "", 
            "title": "Relation Extraction"
        }, 
        {
            "location": "/app_nlp/#sentence-segmentation", 
            "text": "Sentence segmentation is the problem of dividing a string of written language into its component sentences. In English and some other languages, using punctuation, particularly the full stop/period character is a reasonable approximation. However even in English this problem is not trivial due to the use of the full stop character for abbreviations, which may or may not also terminate a sentence.  For example Mr. is not its own sentence in \"Mr. Smith went to the shops in Jones Street.\" When processing plain text, tables of abbreviations that contain periods can help prevent incorrect assignment of sentence boundaries.  As with word segmentation, not all written languages contain punctuation characters which are useful for approximating sentence boundaries.", 
            "title": "Sentence Segmentation"
        }, 
        {
            "location": "/app_nlp/#english-nlp", 
            "text": "Dictionary / Wordnet  1  Corpus   English Wikipedia", 
            "title": "English NLP"
        }, 
        {
            "location": "/app_nlp/#tools", 
            "text": "gensims  wiki", 
            "title": "Tools"
        }, 
        {
            "location": "/app_nlp/#vietnamese-nlp", 
            "text": "H\u1ed3 T\u00fa B\u1ea3o, V\u1ec1 x\u1eed l\u00fd ti\u1ebfng Vi\u1ec7t trong c\u00f4ng ngh\u1ec7 th\u00f4ng tin", 
            "title": "Vietnamese NLP"
        }, 
        {
            "location": "/app_nlp/#part-i-core-problems", 
            "text": "", 
            "title": "Part I. Core Problems"
        }, 
        {
            "location": "/app_nlp/#11-dictionaries", 
            "text": "2004, H\u1ed3 Ng\u1ecdc \u0110\u1ee9c, The Free Vietnamese Dictionary Project", 
            "title": "1.1 Dictionaries"
        }, 
        {
            "location": "/app_nlp/#12-wordnet", 
            "text": "viet wordnet", 
            "title": "1.2 Wordnet"
        }, 
        {
            "location": "/app_nlp/#13-corpus", 
            "text": "VNESEcorpus , 650.000 sentences, 10.000 articles from vietnamnet.vn, dantri.com.vn, nhanhdan.com.vn. Size: 64.59 Mb  VNTQcorpus(small) , 300.000 sentences, 1.000 articles from vnthuquan.net\nSize: ~35 Mb  VNTQcorpus(big) , 1.750.000 sentences, 13.000 articles from vnthuquan.net, Size: ~240 Mb", 
            "title": "1.3 Corpus"
        }, 
        {
            "location": "/app_nlp/#14-sentence-segmentation", 
            "text": "Unknown", 
            "title": "1.4 Sentence Segmentation"
        }, 
        {
            "location": "/app_nlp/#15-word-segmentation", 
            "text": "Vitk   spark  Authors:  Le Hong Phuong \nDate: May 08, 2016  This is the first release of a Vietnamese text processing toolkit, which is called \"Vitk\", developed by Phuong LE-HONG at College of Natural Sciences, Vietnam National University, Hanoi.    vnTokenizer   java  Authors:  Le Hong Phuong \nDate: September 28, 2009  vnTokenizer is a software for tokenizing Vietnamese texts. It segments Vietnamese texts into lexical units (words, names, dates, numbers and other regular expressions) with a high accuracy, of about 98% on a test set extracted from the Vietnamese treebank.     JVnSegmenter   java  Authors: Cam-Tu Nguyen (ncamtu@gmail.com), Xuan-Hieu Phan (pxhieu@gmail.com)\nDate: Mar 24, 2007  A Java-based Vietnamese Word Segmentation Tool    DongDu   C++  Authors: rockkhuya( rockkhuya@gmail.com )  A Vietnamese word segmentation tool.    Roy_VnTokenizer   python  Authors: Anindya Roy\nDate: Jan 22, 2014  Vietnamese tokenization    Online Tool from VLSP   online  Not Available", 
            "title": "1.5 Word Segmentation"
        }, 
        {
            "location": "/app_nlp/#16-part-of-speech-tagging-pos-tagging", 
            "text": "VCCorp 2016: 94.5%  1 \nVitk 2016: accurary 95% (Vietnamese Tree Bank)    Vitk   spark  Authors:  Le Hong Phuong \nDate: May 08, 2016  The part-of-speech tagger of Vitk can tag about 1,105,000 tokens per second, on a single machine, giving an accuracy of about 95% on the Vietnamese treebank.    vnTagger   java  Vietnamese part-of-speech tagging  Authors:  Le Hong Phuong \nDate: Aug 05, 2010  Paper     Le Hong Phuong, 2010, An empirical study of maximum entropy approach for part-of-speech tagging of Vietnamese texts", 
            "title": "1.6 Part-of-speech tagging (POS Tagging)"
        }, 
        {
            "location": "/app_nlp/#17-coreference", 
            "text": "VCCorp 2016: 57%  1  Thesis   Papers :     2011, Gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n \u0111\u1ed3ng tham chi\u1ebfu trong v\u0103n b\u1ea3n ti\u1ebfng vi\u1ec7t d\u1ef1a v\u00e0o ph\u01b0\u01a1ng ph\u00e1p m\u00e1y vector h\u1ed7 tr\u1ee3 SVM", 
            "title": "1.7 Coreference"
        }, 
        {
            "location": "/app_nlp/#18-dependency-grammar", 
            "text": "VCCorp 2016: 73%  1    2013, Nguy\u1ec5n Vi D\u01b0\u01a1ng, Nguy\u1ec5n Th\u1ecb \u0110\u1ea3m, B\u1ed9 chuy\u1ec3n \u0111\u1ed5i t\u1eeb v\u0103n ph\u1ea1m th\u00e0nh ph\u1ea7n sang v\u0103n ph\u1ea1m ph\u1ee5 thu\u1ed9c cho ti\u1ebfng Vi\u1ec7t", 
            "title": "1.8 Dependency Grammar"
        }, 
        {
            "location": "/app_nlp/#19-chunking", 
            "text": "VCCorp 2016: 83%  1", 
            "title": "1.9 Chunking"
        }, 
        {
            "location": "/app_nlp/#110-named-entity-recognition-ner", 
            "text": "VCCorp 2016: 84.8%  1  Papers :     2007, Named Entity Recognition in Vietnamese documents    2005, Named Entity Recognition in Vietnamese Free-Text and Web Documents Using Conditional Random Fields", 
            "title": "1.10 Named Entity Recognition (NER)"
        }, 
        {
            "location": "/app_nlp/#111-relations-extraction-systems", 
            "text": "Unknown", 
            "title": "1.11 Relations Extraction Systems"
        }, 
        {
            "location": "/app_nlp/#112-sentiment-analysis", 
            "text": "Sentiment Analysis   java  Authors:  Epi Lab \nDate: Aug 01, 2013", 
            "title": "1.12 Sentiment Analysis"
        }, 
        {
            "location": "/app_nlp/#113-language-identification", 
            "text": "Unknown", 
            "title": "1.13 Language Identification"
        }, 
        {
            "location": "/app_nlp/#part-2-vietnamse-nlp-groups", 
            "text": "Groups   vnlp.net, (2010-now)  kde lab, (2014-now)   People   Assoc. Prof. Ha Quang Thuy  Prof. Tu-Bao Ho  Khoat Than  Cam Tu Nguyen  CAM-TU NGUYEN  Le Hong Phuong  Phan Xuan Hieu  Tr\u1ea7n Mai V\u0169  Nguy\u1ec5n Ki\u00eam Hi\u1ebfu", 
            "title": "Part 2. Vietnamse NLP Groups"
        }, 
        {
            "location": "/app_nlp/#part-3-applications", 
            "text": "", 
            "title": "Part 3. Applications"
        }, 
        {
            "location": "/app_nlp/#vav-tro-ly-ao-cho-nguoi-viet", 
            "text": "Date: Nov 2015 - now   MDN-Team, Khoa CNTT, Tr\u01b0\u1eddng \u0110H C\u00f4ng ngh\u1ec7, \u0110HQG HN Tools   B\u1ea1n \u0111ang ngh\u0129 \u0111\u1ebfn m\u1ed9t \u1ee9ng d\u1ee5ng th\u00f4ng minh tr\u00ean di \u0111\u1ed9ng cho ph\u00e9p b\u1ea1n t\u01b0\u01a1ng t\u00e1c b\u1eb1ng gi\u1ecdng n\u00f3i \u0111\u1ec3 h\u1eb9n chu\u00f4ng b\u00e1o th\u1ee9c, \u0111\u1eb7t l\u1ecbch cho m\u1ed9t cu\u1ed9c h\u1ecdp, b\u1eadt \u0111\u1ecbnh v\u1ecb, g\u1ecdi \u0111i\u1ec7n cho ai \u0111\u00f3, truy c\u1eadp m\u1ed9t trang web b\u1ea5t k\u1ef3, t\u00ecm \u0111\u01b0\u1eddng tr\u00ean b\u1ea3n \u0111\u1ed3, \u0111\u1ecbnh v\u1ecb c\u00e2y ATM c\u1ee7a m\u1ed9t ng\u00e2n h\u00e0ng n\u00e0o \u0111\u00f3 g\u1ea7n v\u1edbi b\u1ea1n, hay th\u01b0\u1edfng th\u1ee9c m\u1ed9t b\u1ea3n nh\u1ea1c m\u00ecnh y\u00eau th\u00edch \u2026 \u1ee8ng d\u1ee5ng Tr\u1ee3 l\u00fd \u1ea3o VAV ch\u00ednh l\u00e0 c\u00e2u tr\u1ea3 l\u1eddi cho b\u1ea1n.\n\u0110\u01b0\u1ee3c thi\u1ebft k\u1ebf v\u00e0 ph\u00e1t tri\u1ec3n d\u1ef1a tr\u00ean c\u00e1c k\u1ef9 thu\u1eadt tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o (h\u1ecdc m\u00e1y, ph\u00e2n t\u00edch v\u00e0 hi\u1ec3u ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean), VAV c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c \u00fd \u0111\u1ecbnh c\u1ee7a b\u1ea1n d\u00f9 b\u1ea1n di\u1ec5n \u0111\u1ea1t c\u00e2u l\u1ec7nh c\u1ee7a m\u00ecnh theo nhi\u1ec1u c\u00e1ch kh\u00e1c nhau m\u00e0 kh\u00f4ng c\u1ea7n tu\u00e2n theo b\u1ea5t k\u1ef3 khu\u00f4n m\u1eabu n\u00e0o cho tr\u01b0\u1edbc. Nh\u1eefng g\u00ec VAV h\u01b0\u1edbng t\u1edbi l\u00e0 tr\u1edf th\u00e0nh m\u1ed9t tr\u1ee3 l\u00fd \u1ea3o th\u00f4ng minh gi\u00fap b\u1ea1n th\u1ef1c hi\u1ec7n nh\u1eefng \u0111i\u1ec1u m\u00ecnh mu\u1ed1n v\u00e0 l\u00e0 m\u1ed9t ng\u01b0\u1eddi \u0111\u1ed3ng h\u00e0nh th\u00e2n thi\u1ec7n, d\u00ed d\u1ecfm b\u00ean b\u1ea1n.       2016, Big Challenges for Data Scientists at VCCORP    Document classification", 
            "title": "VAV - Tr\u1ee3 l\u00fd \u1ea3o cho ng\u01b0\u1eddi Vi\u1ec7t"
        }, 
        {
            "location": "/app_vision/", 
            "text": "Computer Vision\n\n\nOne-Hot Encoding", 
            "title": "Computer Vision"
        }, 
        {
            "location": "/app_vision/#computer-vision", 
            "text": "One-Hot Encoding", 
            "title": "Computer Vision"
        }, 
        {
            "location": "/qanda/", 
            "text": "Data Science: Questions and Answers\n\n\nWhat is Precision and recall? \n1\n\n\n\n\n\nWhat are 'Overfitting' and 'Underfitting'? \n2\n \n\n\n\nWhat are popular algorithms? \n2\n\n\n\n\n\nDecision Trees\n\n\nNeural Networks (back propagation)\n\n\nProbabilistic networks\n\n\nNearest Neighbor\n\n\nSupport vector machines\n\n\n\n\n\nDescribe a \"Rolls Royce\" solution that you could implement in 3 months, 3 weeks and 3 days.\n\n\n\n[!TODO]\n\n\nMachine Learning Problems \n3\n\n\n\n[!TODO]\n\n\nHow would you generate related searches on Bing? \n3\n\n\n\n[!TODO]\n\n\nHow would you approach the Netflix Prize? \n3\n\n\n\n[!TODO]\n\n\nHow would you suggest followers on Twitter? \n3\n\n\n\n[!TODO]", 
            "title": "Q&A"
        }, 
        {
            "location": "/qanda/#data-science-questions-and-answers", 
            "text": "", 
            "title": "Data Science: Questions and Answers"
        }
    ]
}